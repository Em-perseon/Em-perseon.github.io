
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>XuSibo&#39;s Blog</title>
    <meta name="author" content="Rick" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/avatar.jpg" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>


<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css" />
<script src="/js/lib/math.js"></script>


<script src="/js/lib/preview.js"></script>





<script src="/js/lib/home.js"></script>

<link rel="stylesheet" href="/css/main.css" />

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>XUSIBO&#39;S BLOG</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;XUSIBO&#39;S BLOG</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div id="home-head">
    <div
        id="home-background"
        ref="homeBackground"
        data-images="/images/background.jpg"
    ></div>
    <div id="home-info" @click="homeClick">
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="info">
            <div class="wrap">
                <h1>XuSibo&#39;s Blog</h1>
                <h3></h3>
                <h5></h5>
            </div>
        </span>
    </div>
</div>
<div
    id="home-posts-wrap"
    ref="homePostsWrap"
    true
>
    <div id="home-posts">
        

<div class="post">
    <a href="/2024/07/15/KnowledGPT-Enhancing-Large-Language-Models-with-Retrieval-and-Storage-Access-on/">
        <h2 class="post-title">KnowledGPT Enhancing Large Language Models with Retrieval and Storage Access on</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/7/15
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>大型语言模型(llm)在自然语言处理领域已经显示出令人印象深刻的影响，但它们仍然存在一些问题，如完整性、时效性、信誉度和适应性。虽然最近的努力主要集中在将LLMs与外部知识来源连接起来，但知识库(KBs)的集成仍然没有得到充分的研究，并面临着一些挑战。</p>
<p>在本文中，我们介绍了KnowledgeGPT，这是一个综合框架，可以将LLMs与各种知识库连接起来，促进知识的检索和存储。检索过程采用思想提示程序，生成编码格式的知识库检索语言，并为知识库操作预先定义功能。除了检索之外，KnowledgeGPT还提供了在个性化知识库中存储知识的功能，以满足个人用户的需求。通过大量的实验，我们表明，通过将LLMs与知识库集成，与vanilla LLMs相比，KnowledgeGPT可以正确地回答更多需要世界知识的问题，既利用了众所周知的知识库中存在的知识，也提取到了个性化的知识库中。</p>
<p>文章链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.11761">https://arxiv.org/abs/2308.11761</a></p>
<p>代码链接：</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>现LLMs仍然在考虑完整性、及时性、可靠性和适应性等问题</p>
<ul>
<li>首先，LLMs在特定领域的及时更新和专业知识方面表现出局限性。</li>
<li>其次，这些模型可能产生不忠实或“幻觉”的知识，带来可靠性和伦理问题。</li>
<li>第三，由于成本和可及性等限制，LLMs很难通过持续培训吸收新知识，这阻碍了定制这些模型以适应特定知识需求的能力。</li>
</ul>
<p>本文关注知识库(KBs)，知识库是一种以实体为中心的知识的独特的知识来源形式比如关系三元组和实体描述。</p>
<ul>
<li>一方面各种知识库的构建是为了它们在实际应用中的有效性，以及它们的表示的简洁性、表现力、可解释性和可见性。</li>
<li>另一方面，以前的方法主要集中在文档语料库上，这在应用于KGs时显示出一些不足，如图1所示。</li>
</ul>
<img src="/2024/07/15/KnowledGPT-Enhancing-Large-Language-Models-with-Retrieval-and-Storage-Access-on/image-20240715154241200.png" class="" title="image-20240715154241200">
<p>最近，一些作品试图将llm与KBs联系起来。然而，在这个方向上仍然存在许多挑战，如图2所示。首先，LLMs在知识库中导航复杂和不同问题的过程仍然是一个问题，特别是对于需要在KBs中多个嵌套条目中提供信息的多跳问题。其次，将知识库中的实体和关系与其文本提及对齐是一项具有挑战性的任务，因为它们需要映射到广泛的自然语言表达，并考虑知识库中严重的歧义。第三，尽管知识库中基于三元的表示简洁且可解释，但与自然语言相比，它只涵盖了有限的信息，这表明LLMs需要在知识库中使用新的表示形式。</p>
<img src="/2024/07/15/KnowledGPT-Enhancing-Large-Language-Models-with-Retrieval-and-Storage-Access-on/image-20240715155017808.png" class="" title="image-20240715155017808">
<p>在本文中，我们提出了一个综合框架KnowledgeGPT，将LLMs与各种知识库有效地连接起来，提高了处理复杂问题、歧义和知识表示的能力。KnowledgeGPT实现了一个统一的访问接口，用于对不同KB的操作，包括广泛使用的公共KB和个性化KB内存。</p>
<ul>
<li>KnowledgeGPT访问面向实体的知识，包括实体描述和关系三元组。对于给定的查询，KnowledgeGPT通过三个步骤搜索KBs:搜索代码生成、搜索执行和答案生成。</li>
<li>受(Chen et al .， 2022)的启发，KnowledGPT采用思想程序(program of thoughts, PoT)提示，通过生成Python代码与KBs交互，该代码委托搜索步骤并执行。这段代码封装了评估KBs(例如entity_linking)的函数。</li>
<li>然后，KnowledGPT集成检索到的知识来生成响应。如果KnowledgeGPT认为问题不需要KBs的知识，或者检索到的知识不充分或不存在，则由LLM直接回答问题。</li>
<li>此外，KnowledgeGPT还可以从各种形式的非结构化文本中提取知识，丰富个性化知识库。</li>
</ul>
<p>总结贡献：</p>
<ul>
<li>提出了KnowledgeGPT，一个全面的框架，使法学硕士能够从知识库中检索知识。它极大地推进了法学硕士和知识库之间的合作，以应对复杂搜索和歧义等重要的实际挑战。</li>
<li>建议使用个性化知识库作为llm的符号存储器，将面向实体的知识封装为三种表示形式。与只有三元组的KBs相比，这扩大了符号记忆的知识范围。</li>
<li>用实验证明了提出的方法的有效性。结果强调了使用KBs作为llm符号存储器的效用和潜力。</li>
</ul>
<h1 id="3-Methods"><a href="#3-Methods" class="headerlink" title="3 Methods"></a>3 Methods</h1><h2 id="3-1-任务定义"><a href="#3-1-任务定义" class="headerlink" title="3.1 任务定义"></a>3.1 任务定义</h2><p>KnowledGPT是一个综合性框架，旨在将大型语言模型（LLMs）与各种知识库（KBs）结合起来。它通过补充外部知识来增强LLMs的能力，包括一个个性化的知识库（PKB）作为可写的符号记忆。具体来说，KnowledGPT在处理用户输入的自然语言时，主要承担以下两个任务：</p>
<ol>
<li><strong>知识检索（Knowledge Retrieval）</strong>：模型在提供的知识库中搜索相关信息，以回答用户的查询。</li>
<li><strong>知识存储（Knowledge Storage）</strong>：模型从用户输入中提取知识，并将其插入到个性化知识库中。</li>
</ol>
<h2 id="3-2-知识检索"><a href="#3-2-知识检索" class="headerlink" title="3.2 知识检索"></a>3.2 知识检索</h2><p>KnowledgeGPT采用三步流程，用知识库中的知识回答用户的查询，如图3所示。</p>
<ul>
<li>首先，它生成一段搜索代码，作为特定于查询的KB访问的逻辑形式。</li>
<li>然后，执行搜索代码来检索相关知识。</li>
<li>最后，KnowledGPT读取检索到的知识并回答查询。</li>
</ul>
<img src="/2024/07/15/KnowledGPT-Enhancing-Large-Language-Models-with-Retrieval-and-Storage-Access-on/image-20240715161553479.png" class="" title="image-20240715161553479">
<p>我们利用思想程序(PoT)提示方法(Chen et al .， 2022)，该方法采用Python代码作为llm生成的搜索语言。在本文中，我们使用GPT-4 (OpenAI, 2023)作为LLM。代码封装在一个<strong>搜索函数</strong>中，如图3中黄色部分所示，其中包括内置的Python函数和三个定制的<strong>KB函数</strong>，旨在促进llm与KB的交互:</p>
<ul>
<li>get_entity_info：它接受实体作为输入并返回其百科全书式的描述</li>
<li>find_entity_or_avlue：它接受由实体和关系组成的查询作为输入，并输出相应实体或值的列表。</li>
<li>find_relationship：它接受两个实体作为输入，并返回它们之间关系的列表。</li>
</ul>
<p>特别地，每个实体或关系都表示为候选别名列表，而不是单个名称，以便有效地处理同义词。除了上述输出外，这些KB函数还返回一条记录函数调用和结果的消息。</p>
<p>然后，通过连接来自各个KB函数调用的消息来获得搜索函数的总体输出。</p>
<p>然后执行搜索函数，从KBs中检索期望的知识。代码将在执行前进行装饰，例如，使用try-except语句和特定于kb的访问器对象，如第3.2.1节所述。对每个KB分别并行执行搜索函数，并将它们的结果连接起来。</p>
<p>最后，将检索到的知识提供给llm, llm的任务是响应用户的查询，并得到检索到的知识的支持。</p>
<p>提示符如第7节所示。在llm判断问题不需要外部知识或检索到的知识不足以进行查询的情况下，llm将忽略检索到的信息，并独立处理用户查询</p>
<h3 id="3-2-1-代码应用"><a href="#3-2-1-代码应用" class="headerlink" title="3.2.1 代码应用"></a>3.2.1 代码应用</h3><p>介绍执行生成的代码的KB函数的实现。在两个级别实现这些功能：<strong>统一级别</strong>和<strong>特定于kb的级别</strong>。</p>
<ul>
<li>统一层的功能为不同KBs的操作提供了统一的接口：其中包括生成的三个KB函数(<code>get_entity_info</code>、<code>find_entity_or_value</code>、<code>find_relationship</code>)由LLMs生成，以及一个entity_linking函数，用于将llm生成的实体别名与KBs中的实体对齐。</li>
<li>特定于KB级别的函数通过调用相应的api来实现对每个特定KB的操作。基本上，我们只需要为每个KB实现三个函数：<code>_get_entity_info</code>，<code>_entity_linking</code>，<code>_get_entity_triples</code>。在本文中，我们用下划线来表示这些函数。</li>
<li>在执行之前，我们修饰生成的代码。我们用try-except语句包装代码，这样，如果代码在后续步骤中出现故障，搜索函数仍然会从成功的步骤返回有价值的结果。此外，我们将用户查询作为全局变量传递给搜索函数。</li>
</ul>
<h3 id="3-2-2-实体联系"><a href="#3-2-2-实体联系" class="headerlink" title="3.2.2 实体联系"></a>3.2.2 实体联系</h3><ol>
<li><strong>实体链接的重要性</strong>：<ul>
<li>实体链接是集成大型语言模型（LLMs）与知识库（KBs）的关键步骤。由于一个实体可能有多个别名或提及方式，而一个名词短语也可能指代不同的实体，因此需要确定查询中指的是KB中的哪个具体实体。</li>
</ul>
</li>
<li><strong>实体链接过程</strong>：<ul>
<li>描述了实体链接的三个主要步骤：<ol>
<li>使用KB特定的<code>_entity_linking</code>函数获取候选实体，这涉及到利用实体链接API和搜索API。</li>
<li>调用<code>_get_entity_info</code>函数收集候选实体的信息，并对每条信息进行截断以满足长度限制。</li>
<li>将LLM提供的功能输入（包括查询、实体和关系的别名）和候选实体及其信息一起提供给LLM，让LLM确定最合适的实体。</li>
</ol>
</li>
</ul>
</li>
</ol>
<h3 id="3-2-3-获取实体信息"><a href="#3-2-3-获取实体信息" class="headerlink" title="3.2.3 获取实体信息"></a>3.2.3 获取实体信息</h3><p>介绍了KnowledGPT如何检索和利用知识库（KB）中特定实体的详细信息。</p>
<ol>
<li><strong>函数目的</strong>：<code>get_entity_info</code> 函数的目的是检索KB中与特定实体相关的信息，包括实体的描述和三元组信息。</li>
<li><strong>实体链接</strong>：函数首先使用实体链接步骤来将输入的实体别名与KB中的实体对应起来，这是通过在前一节3.2.2中讨论的<code>entity_linking</code>函数实现的。</li>
<li><strong>调用KB特定函数</strong>：一旦确定了正确的实体，<code>get_entity_info</code> 函数会调用KB特定的<code>_get_entity_info</code> 函数来获取该实体的详细信息。</li>
<li><strong>信息内容</strong>：获取的信息通常包括实体的描述（例如，实体是谁或是什么）和与实体相关的三元组（即实体的属性或与其他实体的关系）。</li>
<li><strong>处理三元组</strong>：通过调用<code>_get_entity_triples</code> 函数来收集与实体相关的所有三元组。三元组是知识库中表示知识的基本单位，通常包括实体、关系和值。</li>
<li><strong>信息截断</strong>：对于收集到的每条实体信息，可能会进行截断以满足长度限制，确保信息简洁且相关。</li>
<li><strong>信息整合</strong>：将从KB检索到的信息整合成一种格式，使其可以被LLMs用来生成对用户查询的回答。</li>
<li><strong>上下文使用</strong>：描述了如何将检索到的实体信息用于回答查询，包括在多跳查询中如何利用这些信息来构建更复杂的答案。</li>
</ol>
<h3 id="3-2-4-找到实体或者值"><a href="#3-2-4-找到实体或者值" class="headerlink" title="3.2.4 找到实体或者值"></a>3.2.4 找到实体或者值</h3><p>介绍了KnowledGPT如何根据用户查询中的实体和关系来检索相应的实体或属性值。</p>
<ol>
<li><strong>功能目的</strong>：<ul>
<li><code>find_entity_or_value</code> 函数旨在根据给定的查询，检索与特定实体和关系相关的实体或值。这可以用于回答涉及特定属性或与其他实体关系的查询。</li>
</ul>
</li>
<li><strong>处理流程</strong>：<ul>
<li>函数通过一系列步骤来处理查询，包括调用实体链接函数来确定查询中提及的实体，然后搜索与该实体相关的信息。<ul>
<li><strong>实体链接</strong>：<ul>
<li>在开始检索之前，首先使用实体链接来将查询中的实体别名与知识库中的实体对应起来。</li>
</ul>
</li>
<li><strong>检索三元组</strong>：<ul>
<li>调用内部的 <code>_find_entity_or_value</code> 函数，该函数负责检索与实体相关的所有三元组。三元组是知识库中用来表示实体属性或关系的一组三个元素（主体、谓词、对象）。</li>
</ul>
</li>
<li><strong>关系相似度</strong>：<ul>
<li>对检索到的三元组中的关系，根据它们与查询中的关系别名的相似度进行排序。这里使用句子嵌入的余弦相似度，而不是符号度量，以便考虑关系的同义词。</li>
</ul>
</li>
<li><strong>选择最相关的关系</strong>：<ul>
<li>选择与输入关系别名相似度最高的三元组，并返回相应的实体或属性值。</li>
</ul>
</li>
<li><strong>描述搜索</strong>：<ul>
<li>如果没有找到相关的三元组，函数会在实体描述中进一步搜索关系，如果关系在描述中出现，则返回对应的句子；如果没有找到，返回整个描述。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="3-2-5-找到关系"><a href="#3-2-5-找到关系" class="headerlink" title="3.2.5 找到关系"></a>3.2.5 找到关系</h3><p>介绍了KnowledGPT如何检索两个实体之间的关系</p>
<ol>
<li><p><strong>功能目的</strong>：</p>
<ul>
<li><code>find_relationship</code> 函数的目标是确定用户查询中提到的两个实体之间的联系或关系。</li>
</ul>
</li>
<li><p><strong>检索流程</strong>：</p>
<ul>
<li>该函数通过检索与第一个实体相关的信息，然后寻找第二个实体的信息来确定它们之间的关系。<ul>
<li><strong>与<code>find_entity_or_value</code>的相似性</strong>：<ul>
<li><code>find_relationship</code> 函数在操作方式上与 <code>find_entity_or_value</code> 类似，但区别在于它在检索过程中寻找第二个实体而不是关系。</li>
</ul>
</li>
<li><strong>处理失败的搜索</strong>：<ul>
<li>如果初次搜索未能找到第二个实体，函数会交换两个实体的位置并再次进行搜索，以确定它们之间的关系。</li>
</ul>
</li>
<li><strong>实体相似度度量</strong>：<ul>
<li>与关系相似度使用余弦相似度不同，实体相似度通过Levenshtein距离来计算，这是一种衡量两个序列编辑距离的方法。</li>
</ul>
</li>
<li><strong>相似度计算</strong>：<ul>
<li>实体相似度的计算方法为：如果两个实体名称有重叠的单词，则相似度为 100 减去编辑距离；如果没有重叠，则相似度为 0。</li>
</ul>
</li>
<li><strong>多跳搜索</strong>：<ul>
<li>描述了如何通过多跳搜索来处理复杂的关系查询，这可能涉及到在知识库中进行多次检索以找到间接关系。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="3-3-知识存储"><a href="#3-3-知识存储" class="headerlink" title="3.3 知识存储"></a>3.3 知识存储</h2><p>介绍了 KnowledGPT 如何存储知识</p>
<p>尽管公共知识库（KB）提供了丰富的世界知识，但它们仍无法覆盖用户感兴趣的所有知识。为了满足用户个人的知识需求，KnowledGPT 引入了一个<strong>个性化的知识库</strong>（Personalized KB，简称 PKB），作为大型语言模型（LLMs）的符号化记忆，使用户能够存储和访问专门知识。PKB 通过从用户提供的文档中提取知识来填充。当用户打算将知识添加到 PKB 时，系统会提示 LLMs 从他们提供的文档中提取知识。</p>
<p>KnowledGPT 考虑了三种形式的知识表示，包括实体描述、关系三元组和实体方面信息，如图 2 所示。与仅提取三元组的 RET-LLM（Modarressi 等人，2023 年）或 LangChain 的 KG-Index（Chase，2022 年）和 Llama Index（Liu，2022 年）不同。实体描述和关系三元组已在 Wikipedia 和 Wikidata 等知识库中广泛采用，但它们仅表示了有限部分的知识。例如，如果我们想了解苏格拉底作为士兵的经历，苏格拉底 Wikipedia 页面上的大部分内容可能几乎没有帮助，而且这些内容也很难表示为三元组。因此，作者们提出了一种称为实体方面信息的额外知识表示形式，作为 LLMs 符号记忆的变体。这是一种三元组的变化，其中对象是一段长文本，通过实体和一个方面来描述和检索。例如，一个记录可能被索引为（”苏格拉底”，”军事服务”），对应于描述 “苏格拉底曾作为希腊重装步兵……”。以这种形式表示的知识也将通过 get_entity_or_value 函数被检索。</p>
<p>鉴于 PKB 的规模与公共 KB 相比很小，作者们考虑了 PKB 实体链接的不同策略。主要区别有三方面：</p>
<ul>
<li>首先，基于确切匹配和嵌入相似性为 PKB 定义了实体搜索 API。嵌入相似性有助于识别众所周知的实体别名，例如 Chanelle Scott Calica 和 Shystie。</li>
<li>其次，在提取过程中，提取的实体提及不会与 PKB 中现有的实体对齐。因此，一个实体可能在不同文档中被提取为不同的提及。因此，对于实体链接，KnowledGPT 返回多个匹配的实体。</li>
<li>第三，实体将带有一个别名列表被提取，并提供给 LLMs 用于实体链接。对于 get_entity_or_value 函数，由于关系也可以作为不同的表达式提取，我们选择检索超过某个阈值相似度分数的关系，而不是仅检索最高分的关系。</li>
</ul>
<h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h1><h2 id="4-1-实验设置"><a href="#4-1-实验设置" class="headerlink" title="4.1 实验设置"></a>4.1 实验设置</h2><p>描述了用于实验的知识库和语言模型。知识库包括 Wikipedia、Wikidata、CN-DBPedia 和个性化知识库（PKB）。语言模型主要使用 OpenAI 提供的 GPT-4。</p>
<h2 id="4-2-对流行知识库的查询"><a href="#4-2-对流行知识库的查询" class="headerlink" title="4.2 对流行知识库的查询"></a>4.2 对流行知识库的查询</h2><p>手工制作了一组问题，这些问题需要从 CN-DBPedia 获取知识。这些问题包括单跳和多跳关系查询、关系预测、多样化指令、混合查询和值比较。实验结果表明，GPT-4 和 ChatGPT 在不需要知识库的情况下直接回答问题时表现良好，但当涉及到不太知名的实体时，它们也常常产生错误信息。KnowledGPT 显著提高了任务的成功率，尤其是在代码生成和实体链接方面。</p>
<h2 id="4-3-基于知识的问答"><a href="#4-3-基于知识的问答" class="headerlink" title="4.3 基于知识的问答"></a>4.3 基于知识的问答</h2><p>评估了 KnowledGPT 在零样本知识库问答（zero-shot KBQA）上的性能。他们创建了两个数据集，NLPCC-100 用于单跳查询，NLPCC-MH-59 用于多跳查询。KnowledGPT 在这两个数据集上的表现超过了基于 BM25 和嵌入相似性的检索方法，以及之前在 NLPCC-2016 KBQA 数据集上训练的 SPE 方法。</p>
<h2 id="4-4-作为记忆的知识库"><a href="#4-4-作为记忆的知识库" class="headerlink" title="4.4 作为记忆的知识库"></a>4.4 作为记忆的知识库</h2><p>研究了 KnowledGPT 结合可修改的个性化知识库（PKB）的效果。KnowledGPT 从提供的文档中提取知识以构建 PKB，并使用 PKB 回答相关问题。实验结果表明，KnowledGPT 能够成功回答所有比较问题和大部分桥梁问题。此外，作者们还研究了 KnowledGPT 在提取 HotpotQA 数据集中 100 个文档上的知识覆盖率，发现当包括实体描述和实体方面信息时，知识提取覆盖率有显著提高。</p>
<h1 id="5-缺陷"><a href="#5-缺陷" class="headerlink" title="5 缺陷"></a>5 缺陷</h1><p>尽管 KnowledGPT 在实验中表现出色，但仍存在一些限制。例如，检索过程只进行了一轮代码生成和执行，可能需要多轮机制来更好地允许 LLMs 自主探索知识库。此外，实验是在代表性但较小的数据集上进行的，受限于通过 API 访问 GPT-4 的成本。作者们还计划在未来的工作中研究对 LLMs 如 Llama 进行微调以提高效率，并进行更全面的实验。</p>

            
        </div>
    </div>
    <div class="post-tags">
        
        
        
    </div>
    <a href="/2024/07/15/KnowledGPT-Enhancing-Large-Language-Models-with-Retrieval-and-Storage-Access-on/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/07/14/From-Supervised-to-Generative-A-Novel-Paradigm-for-Tabular-Deep-Learning-with-Large-Language-Models/">
        <h2 class="post-title">From Supervised to Generative A Novel Paradigm for Tabular Deep Learning with Large Language Models</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/7/14
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p>[TOC]</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>表格数据是各种关键行业预测建模的基础，包括医疗保健、金融、零售、可持续发展等。<ul>
<li>表格数据就是以二维表格形式展示的数据</li>
</ul>
</li>
<li>尽管在专业模型方面取得了进展，但对通用模型的需求不断增加，这种模型可以转移知识，从有限的数据中进行推广，并遵循人类的指示。这些都是当前表格深度学习方法尚未完全解决的挑战。</li>
<li>本文介绍生成式表格学习(GenerativeTabular Learning GTL)，这是一个新的框架，它将大型语言模型(llm)的高级功能(如基于提示的零概率泛化和上下文学习)集成到表格深度学习中<ul>
<li>GTL利用LLMs对各种表格数据的预训练，增强他们对特定领域知识、数字序列和对准确预测至关重要的统计依赖性的理解。</li>
</ul>
</li>
</ul>
<p>文章链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.07338">https://arxiv.org/abs/2310.07338</a></p>
<p>代码链接：<a target="_blank" rel="noopener" href="https://github.com/microsoft/Industrial-Foundation-Models">https://github.com/microsoft/Industrial-Foundation-Models</a></p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>表格数据广泛存在于医疗保健、金融、零售、可持续发展和气候等众多关键行业领域，是预测建模的基石。这种建模支持多种现实世界的应用，包括疾病风险分层、信用评估、销量预测、电网稳定性测量、气候估计等。鉴于其重要性，它吸引了机器学习社区的大量研究关注</p>
<p>虽然在为表格数据中的单个任务开发专门的预测模型方面取得了相当大的进展，采用有效的学习模型，如基于梯度增强决策树的树集成模型和最近的神经网络，但<strong>跨各个领域的丰富多样的应用场景强调了对通用模型的迫切需求</strong>。</p>
<p>这些模型应该具有无缝转换到新数据集的能力，并表现出强大的泛化能力，特别是在具有少量标签的场景中。</p>
<p>这种必要性催化了<strong>表格深度学习</strong>作为研究焦点的出现，它探索了在广泛的表格数据上预训练通用神经网络的概念。这种方法旨在创建模型，一旦进行预训练，就可以毫不费力地适应广泛的任务，从而提高它们在不同应用中的效用和效率</p>
<ul>
<li>这种形式的适应不仅是用户友好的，而且还极大地增强了模型将在预训练期间获得的知识有效地转移到新的、低资源场景的能力。</li>
<li>其次，在涉及少量标签的情况下，这些模型表现出有趣的上下文学习能力，这可以通过它们从提示中包含的演示中学习的令人印象深刻的能力来证明</li>
<li>这种无需微调的能力隐含地利用贝叶斯推理，在前向传播过程中将模型的预训练知识直接扩展到新的数据样本。</li>
</ul>
<p>但是，大多数现有的表格数据预训练方法主要坚持监督范式，需要后续的微调过程来适应新的表格任务。</p>
<p>尽管开创性的努力通过先进的泛化能力来超越这些限制，即对未见任务的零学习和基于新表格实例的上下文学习，但结果往往封装了受限的泛化能力和有限的应用范围。现有模型的有限范围强调了对更全面的表格深度学习范式的需求，这种范式不仅包含而且扩展了在其他前沿基础模型中发现的高级泛化能力。</p>
<p>然而，将现代大型语言模型的复杂功能(特别是基于提示的零概率泛化和上下文学习)集成到表格深度学习中存在重大挑战。</p>
<ul>
<li>一个主要的问题是，LLMs主要是预先接受过语言数据的训练，他们常常很难完全掌握语言格式的表格数据的细微差别。尽管这些模型能够熟练地从文本语料库中获取广泛的世界知识和高级推理技能，但它们往往缺乏理解特定领域知识的能力，而这对于有效的表格深度学习至关重要。这种不足在它们对表示为标记序列的数字特征的处理，以及在识别预测目标和表格特征之间存在的复杂统计依赖关系，以及在上下文演示和预测数据样本之间存在的依赖关系方面尤为明显。这些能力对于理解表格数据集的独特特征并利用它们进行准确的预测建模至关重要。</li>
<li>为了解决这一缺点，我们引入了生成式表格学习(GTL)——一种新的范式，提倡对LLMs在广泛的表格数据上进行持续的预训练，这些数据以面向教学的语言格式转录，跨越多个领域。</li>
<li>GTL是精心设计的，旨在增强对表格特征的理解，它具有与预测目标和上下文数据示例的相互关系，以及任务指令和表格预测之间的联系。为了简化这个过程，我们创建了特定的文本模板，将表格数据实例转换为面向指令的语言格式。这种转换迎合了各种配置，例如是否包括元信息和上下文示例，并维护数字令牌和目标令牌的位置记录。因此，GTL为模型配备了为表格深度学习量身定制的高级指令跟随能力，使其能够通过解释新任务指令或上下文示例的自然语言提示来生成下游任务。</li>
</ul>
<p>总结：</p>
<ul>
<li>提出了GTL，这是一种生成式学习范式，将LLMs的高级功能(特别是基于提示的零次泛化和上下文学习)扩展到表格深度学习领域。GTL独特地增强了模型对表格数据的理解，通过继续在以指令为导向的语言格式化的各种表格数据集上进行预训练过程，解决了当前方法中的关键差距。</li>
<li>为了支持GTL，我们开发了一个全面的数据构造管道，将表格实例转换为面向指令的语言格式。这条管道不仅促进了对表格深度学习中指令跟随能力的进一步研究，而且丰富了LLMs可用的培训和评估资源，旨在提高他们对表格数据的理解和预测准确性。</li>
<li>为了证明GTL的有效性，我们训练了LLaMA-GTL模型，在表格数据的各种分类和回归任务中展示了其出色的零射击和上下文学习性能。LLaMA-GTL为表格模型的适应性和泛化设定了新的基准，在<strong>特定任务</strong>中超越了传统方法，甚至超越了GPT-4等最先进的llm。<ul>
<li><strong>Zero-shot学习</strong>（Zero-shot Learning，ZSL）是深度学习领域中的一个重要研究方向，它旨在让机器学习模型能够在没有见过特定类别样本的情况下，对该类别进行识别或分类。换句话说，ZSL试图让模型能够泛化到训练时未见过的类别上。</li>
</ul>
</li>
</ul>
<h1 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2 Related work"></a>2 Related work</h1><h1 id="3-大型语言模型的生成表学习"><a href="#3-大型语言模型的生成表学习" class="headerlink" title="3 大型语言模型的生成表学习"></a>3 大型语言模型的生成表学习</h1><h2 id="3-1-问题形成"><a href="#3-1-问题形成" class="headerlink" title="3.1 问题形成"></a>3.1 问题形成</h2><ul>
<li><p><strong>符号：</strong>在表学习领域当中，我们通常处理表格任务$\mathcal{T}:\mathcal{X}\to\mathcal{Y}$，这个和一个表例$x\in \mathcal{X}$相联系，这个表由$M$个特征组成，$\{x_i\}_{i=1}^M$。同时还有一个预测目标，$y \in \mathcal{Y}$。对于回归任务，$y\subseteq\mathbb{R}^1$；对于分类任务，$\mathcal{Y}=\{0,1,…,C-1\}$，还将任务$\mathcal{T}$的<strong>综合元信息</strong>表示为$\mathcal{M}^{\mathcal{T}}$。传统的表格式数据学习方法主要侧重于利用训练数据构建一个判别模型来学习目标与特征之间的依赖关系$p(y|x)$，然而，这种与训练数据紧密相关的模型在适应<strong>具有不同数据模式($\widetilde{\mathcal{X}}$)和预测目标($\widetilde{\mathcal{Y}}$)的新表格任务($\widetilde{\mathcal{T}}$​)</strong>时面临重大挑战。</p>
<ul>
<li><strong>元信息（Metadata）</strong>是关于数据的数据。它提供了关于数据本身的描述、上下文和管理信息，帮助用户理解、使用和管理数据。元信息的主要功能是描述和解释数据的内容、结构和属性，增加数据的可理解性和可用性。</li>
<li>注意，表格特征通常分为数值类型和分类类型，而我们在这里没有明确区分它们。此外，每个表格任务还可能与各种元信息元素相关联，如任务背景、预测目标解释和特征描述。</li>
</ul>
</li>
<li><p><strong>Zero-shot learning：</strong>在表格数据领域，零射击学习的典型特征是能够预测来自以前未见过的任务$\widetilde{\mathcal{T}}$的数据样本的结果。这个新任务具有未知的数据模式$\widetilde{\mathcal{X}}$，但它具有元信息$\mathcal{M}^{\widetilde{\mathcal{T}}}$，它<strong>封装了特征和预测目标的含义</strong>。形式上，我们定义了一个<strong>零点射击的学习任务</strong>作为一个元组$(\mathcal{M}^{\widetilde{\mathcal{T}}},\widetilde{\mathcal{x}}^{new})$，其中$\mathcal{M}^{\widetilde{\mathcal{T}}}$代表了新任务的元信息，$\widetilde{\mathcal{x}}^{new}$​代表了一个新我们想要预测的新的数据样本</p>
<ul>
<li><strong>Zero-shot Learning（零样本学习）</strong>是一种机器学习技术，其目标是在没有看到过某些类别的样本（即零样本）的情况下，能够识别和分类这些新类别。与传统的机器学习方法依赖于大量标注的训练数据不同，零样本学习利用类别之间的关系和特征来推断新类别。</li>
</ul>
</li>
<li><p><strong>In-context Learning：</strong>上下文学习类似Zero-shot learning，但有一个额外的维度，它包括一组来自新任务$\widetilde{\mathcal{T}}$的示例作为演示，表示为<script type="math/tex">\mathcal{D}^{\widetilde{\mathcal{T}}}=\{\widetilde{x}^j,\widetilde{y}^j\}_{j=1}^N</script>，其中$N$象征着上下文示例的数量。将上下文学习任务的输入表示为元组$(\mathcal{M}^{\widetilde{\mathcal{T}}},\mathcal{D}^{\widetilde{\mathcal{T}}},\widetilde{\mathcal{x}}^{new})$，其中元信息$\mathcal{M}^{\widetilde{\mathcal{T}}}$可以省略，如果模型不利用他，那么通过允许模型在$\widetilde{x}^{new}$上进行预测之前在$D^{\widetilde{\mathcal{T}}}$​上进行进一步的微调，上下文学习可以过渡到few-shot learning</p>
<ul>
<li>In-context Learning（上下文学习）和Zero-shot Learning（零样本学习）都是在没有直接见过新类别或任务的情况下进行预测的机器学习方法，但它们有一些关键的区别，其中最显著的区别在于In-context Learning<strong>引入了“上下文”这一额外维度</strong>。具体来说，In-context Learning利用了在推理过程中提供的额外信息或示例，而Zero-shot Learning则通常依赖于预先学到的通用知识或特征表示。以下是</li>
<li>Few-shot learning (FSL) 是一种机器学习方法，旨在用极少量的训练样本进行有效的学习和预测。这种方法特别适用于数据稀缺或难以获取大量标注数据的场景，如医疗图像分析、稀有物种识别等。</li>
</ul>
</li>
</ul>
<h2 id="3-2-面向教学语言格式的表格数据构造-Construction-of-Tabular-Data-in-an-Instruction-Oriented-Language-Format"><a href="#3-2-面向教学语言格式的表格数据构造-Construction-of-Tabular-Data-in-an-Instruction-Oriented-Language-Format" class="headerlink" title="3.2 面向教学语言格式的表格数据构造(Construction of Tabular Data in an Instruction-Oriented Language Format)"></a>3.2 面向教学语言格式的表格数据构造(Construction of Tabular Data in an Instruction-Oriented Language Format)</h2><img src="/2024/07/14/From-Supervised-to-Generative-A-Novel-Paradigm-for-Tabular-Deep-Learning-with-Large-Language-Models/image-20240715092817829.png" class="" title="image-20240715092817829">
<p>图1的上半部分描述了用于数据构造的管道。</p>
<ol>
<li><p>我们的初步步骤是收集大量的表格数据集，这些数据集涵盖了跨多个领域的广泛预测任务（这个过程，在4.1节中有更多的细节）需要同时获取特征和标签，最好包括元信息(可选)</p>
</li>
<li><p>然后，对于每个指定的表格任务$\mathcal{T}$和任何附带的元信息$\mathcal{M}^{\mathcal{T}}$，有必要从原始表格中转换数据样本格式为面向指令的语言格式，以支持后续的GTL过程。这种格式主要由三个部分组成。</p>
</li>
</ol>
<ul>
<li><strong>任务指令</strong>：详细说明预测任务的背景和目标。</li>
<li><strong>特征描述</strong>：包括它们的值和相关含义（当可用时）。可以在此部分中包含上下文示例，并引入提示表示上下文示例的存在。</li>
<li><strong>答案描述</strong>：通常以答案指令开始，后跟预测目标</li>
</ul>
<ol>
<li>遵循这些指导方针，我们设计了各种模板，以满足表达功能描述和不同需求的不同方式。</li>
</ol>
<ul>
<li><p><strong>T-lang 模板</strong>：将表格特征以自然语言风格描述，模仿人类语言风格，有助于知识转移。</p>
<ul>
<li><strong>自然语言风格</strong>：”T-lang” 模板通过将每个特征转换成句子的形式，保持了数据样本的语义信息。这种风格模仿人类的语言表达方式，有助于提高大型语言模型（LLMs）对数据的理解和知识转移。</li>
<li><strong>特征描述</strong>：模板为表格中的每个特征提供了详细的描述，包括它们的值和相关含义（如果可用）。这有助于模型更好地理解每个特征的重要性和上下文。</li>
<li><strong>任务指令</strong>：”T-lang” 模板包含有关预测任务的背景和目标的说明，为模型提供了执行任务所需的上下文。</li>
<li><strong>答案提示</strong>：模板设计了答案提示部分，通常以答案指令开始，后跟预测目标，指导模型如何根据给定的特征生成预测结果。</li>
<li><strong>上下文示例</strong>：”T-lang” 模板可以包含上下文示例，这些示例作为参考，帮助模型理解如何根据特征值进行预测。</li>
</ul>
</li>
<li><p><strong>T-table 模板</strong>：使用 Markdown 表格格式来封装表格特征值，高效处理表格数据，尤其是在上下文学习设置中。</p>
<ul>
<li><strong>表格格式保持</strong>：”T-table” 模板保留了数据的原始表格布局，使用户和模型都能够清晰地看到数据的行列结构。</li>
<li><strong>元信息附加</strong>：在数据之前添加了元信息，如特征描述和标签描述，但这些信息不会在每个上下文示例中重复出现。这有助于模型理解数据的背景和上下文。</li>
<li><strong>Markdown 表格</strong>：使用 Markdown 格式来组织和展示表格数据，使得数据在视觉上更加清晰，同时也方便了电子文档的阅读和处理。</li>
<li><strong>特征和标签的描述</strong>：在表格中，特征和标签的描述被清晰地列出，帮助模型和用户理解每个数据字段的含义。</li>
<li><strong>上下文示例的整合</strong>：”T-table” 模板可以整合上下文示例，这些示例提供了具体的数据点，帮助模型学习如何根据特征进行预测。</li>
</ul>
<img src="/2024/07/14/From-Supervised-to-Generative-A-Novel-Paradigm-for-Tabular-Deep-Learning-with-Large-Language-Models/image-20240715101824358.png" class="" title="image-20240715101824358">
</li>
<li><p><strong>T-anony 模板</strong>：是 T-table 模板的变体，省略了所有元信息，适用于缺乏背景或特征含义知识的实际场景。</p>
<ul>
<li><strong>表格数据的结构</strong>：即使没有元信息，”T-anony” 模板仍然保持了表格数据的基本结构，如行列的布局和数据的组织形式。</li>
<li><strong>特征值</strong>：模板中仍然包含了表格中的实际数据（特征值），这些是进行预测和分析的基础。</li>
<li><strong>Markdown 格式</strong>：”T-anony” 模板使用 Markdown 格式来组织数据，使得数据即使在缺少描述性元信息的情况下也能清晰地呈现。</li>
<li><strong>预测任务的指示</strong>：即便没有详细的背景信息，模板中可能仍然包含对于模型进行预测任务的基本指示。</li>
<li><strong>上下文示例</strong>：如果适用，”T-anony” 模板可以包括一些上下文示例，这些示例提供了数据的用法，但不会包含关于特征或标签的详细描述。</li>
<li><strong>匿名化处理</strong>：通过省略元信息，”T-anony” 模板允许模型在没有任何先验知识的情况下，仅依靠数据本身的特征值来进行学习和预测。</li>
</ul>
</li>
</ul>
<ol>
<li><strong>模板应用</strong>：利用这些模板，可以适应不同领域中的各种表格任务和数据模式。同时，当可用时，可以利用与这些表格数据集相关的元信息，并且在没有此类信息的情况下也有相应的机制。</li>
<li><strong>零样本学习和上下文学习</strong>：通过这些模板，可以轻松地融入零样本学习和上下文学习场景，通过调整上下文示例的数量来适应不同的学习情况。</li>
</ol>
<h2 id="3-3-学习和适应"><a href="#3-3-学习和适应" class="headerlink" title="3.3 学习和适应"></a>3.3 学习和适应</h2><p>图1的底部直观地描述了GTL的训练和适应。</p>
<p><strong>目标（Objective of GTL）</strong>:</p>
<ul>
<li>GTL 的目标是训练大型语言模型（LLMs），<strong>使其能够理解表格数据的复杂结构和语义</strong>，以及如何根据这些数据生成预测结果。GTL 通过预训练和微调过程，增强模型对表格数据的理解和预测能力。</li>
</ul>
<p><strong>标记化指令导向型语言数据</strong>：描述了如何将<strong>指令导向型语言数据</strong>（例如，使用 T-lang 模板转换的表格数据）进行<strong>标记化</strong>（tokenization），以便用于模型训练。这包括任务指令、特征描述、和答案提示的标记化表示。</p>
<ul>
<li>标记化（Tokenization）是自然语言处理（NLP）中的一个基本步骤，主要是<strong>将文本分割成更小的单元</strong>，这些单元称为标记（tokens）。这些标记可以是单词、子词、字符或句子，具体取决于应用场景和使用的标记化方法。</li>
</ul>
<ol>
<li><strong>任务指令标记化</strong>将<strong>第一部分</strong>的标记化结果表示为：$t^b=[t_1^b,…,t^b_{|t^b|}]$，其中$|·|$表示序列的长度。</li>
<li><p><strong>特征标记化</strong>，将$i-th$长度的特征表示为$t^{f_i}=[l^{fi},t^{x_i},r^{f_i}]$</p>
<ol>
<li>$l^{f_i}=[l^{f_i}_1,…,l^{f_i}_{|l^{f_i}|}]$，特征描述的左边部分</li>
<li>$t^{x_i}=[t^{x_i},…,t^{x_i}_{|t^{x_i}|}]$，表示特征值$x_i$的符号序列</li>
<li>$r^{f_i}=[r^{f_i}_1,…,r^{f_i}_{|r^{f_i}|}]，$​特征描述的右边部分</li>
<li>比如处理”The quick brown fox jumps over the lazy dog.” ，将”for”作为特征值的化<ol>
<li>左边部分就是The quick brown</li>
<li>右边部分就是jumps over the lazy dog.</li>
</ol>
</li>
</ol>
</li>
<li><p><strong>回答提示标记化</strong>通过连接所有特征的标记序列，我们得到的整体特征序列为：$t^f=[t^{f_1},…t^{f_M}]$。最后，回答提示为$t^{\mathcal{a}}=[l^{\mathcal{a}},t^y]$，其中$l^a=[l^a_1,…,l^a_{|l^a|}]$代表代表了问题或任务的描述部分。$t^y=[t^y_1,…,t^y_{|t^y|}]$代表了答案的具体内容或预测目标。</p>
</li>
<li>这样，我们可以表达一个表示例$x$和对应的目标值$y$作为一个序列令牌</li>
</ol>
<script type="math/tex; mode=display">[t^b,t^f,t^a]=[t^b,l^{f_1},t^{x_1},r^{f_1},...,l^{f_M},t^{x_M},r^{f_M},t^y]</script><ul>
<li><p>系统地集合任务背景$(t^b)$，特征意义$(\{l^{f_i},r_{f_i}\}^M_{i=1})$和特征值$(\{t^{x_i}\}^M_{i=1})$并且支持不同的预测目标通过一个变长序列$(t^y)$​。</p>
</li>
<li><p>举个例子：</p>
<ul>
<li><p>“小明昨天买了一只新手机。”</p>
<ul>
<li><p>任务指令标记化 ($t^b$​)：任务指令标记化是将任务或指令描述部分标记化为一个序列。</p>
<ul>
<li>$t^b$ 序列为：$t^b=[t_1^b,…,t^b_{|t^b|}]$</li>
</ul>
<p>对于任务指令 “提取时间信息”，$t^b$ 可能会表示为$t^b$​=[“提取”,”时间”,”信息”]</p>
</li>
<li><p>特征标记化 ($t^{f_i}$​)：特征标记化是将每个特征描述为一个序列，包括左边部分、特征值的符号序列和右边部分。</p>
<ul>
<li>假设要将 “昨天” 作为特征值化，我们分析其左右两侧的内容。<ul>
<li>左边部分 $l^{f_i}$：[“小明”]</li>
<li>特征值符号序列 $t^{x_i}$：[“昨天”]</li>
<li>右边部分 $r^{f_i}$：[“买了一只新手机”]</li>
</ul>
</li>
</ul>
</li>
<li><p>回答提示标记化 ($t^{\mathcal{a}}$)：回答提示标记化是将问题或任务的描述部分和答案的具体内容或预测目标标记化为序列。</p>
<ul>
<li><p>假设问题是 “小明做了什么？”</p>
<ul>
<li>问题描述部分 $l^{\mathcal{a}}$​：[“小明”,”做了”,”什么”]</li>
<li>答案内容或预测目标 $t^y$​：[“买了一只新手机”]</li>
</ul>
</li>
<li><p>所以，回答提示标记化序列 $t^{\mathcal{a}}$ 可以表示为：[[“小明”,”做了”,”什么”],[“买了一只新手机”]]</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol>
<li>请注意，上面的标记化符号也适用于其他模板，比如T-table，只是用不同的方式指定特征含义。此外，我们需要标记当前表格数据样本的特征值标记和预测目标标记的位置，以支持后续的GTL过程。</li>
</ol>
<p><strong>GTL 的联合分布（Joint Distribution in GTL）</strong>:GTL 通过定义一个联合分布来表征表格实例的特征和目标值之间的关系。这个分布考虑了所有元信息，并尝试预测给定特征的目标值。</p>
<script type="math/tex; mode=display">p(x,y)=p(t^x,t^y|t^m)=p(t^y|t^x,t^m)\prod_{i=1}^Mp(t^{x_i}|t^{<x_i})</script><ul>
<li>引入额外的符号，以确保公式$t^{m} = [t^{b},l^{f_{1}},r^{f_{1}},\cdots,l^{f_{M}},r^{f_{M}},l^{a}]$，代表所有元信息，$t^{x}=[t^{x_{1}},\cdots,t^{x_{M}}]$代表所有标记相关的特征值，并且$t^{&lt;x_i}$包含所有$[t^b,t^f,t^a]=[t^b,l^{f_1},t^{x_1},r^{f_1},…,l^{f_M},t^{x_M},r^{f_M},t^y]$等式中$t^{x_1}$​前面的令牌</li>
<li>作用和意义</li>
</ul>
<ol>
<li>这里$p(x,y)$代表在初始特征和标签空间的联合分布，而$p(t^x,t^y|t^m)$表示相同的联合分布条件在所有元信息使用文本表示，这可以进一步解耦自回归为$p(t^y|t^x,t^m)\prod_{i=1}^Mp(t^{x_i}|t^{&lt;x_i})$，​</li>
<li>因此，利用llm，特别是那些使用自回归体系结构的llm，来描述这种解耦公式是很直接的。唯一需要的修改是掩盖元信息令牌上的损失。虽然GTL使用类似于llm的下一个令牌预测损失，但它与自回归预训练不同,。具体来说，GTL明确地刺激llm识别预测目标令牌和功能令牌之间的复杂依赖关系。它鼓励捕获当前特征和上下文示例之间的复杂关系，并促进建立不同语言指令和数值数据之间的有效联系</li>
</ol>
<p><strong>适应下游任务：</strong>是指在机器学习或深度学习中，将一个已经经过预训练的模型（通常是在大规模数据上训练的）应用于特定的目标任务或领域时，进行的调整和优化过程。</p>
<ul>
<li><p>对于一个LLM，我们将其在GTL过程之后的变体称为LLM-GTL。</p>
</li>
<li><p>在适应新任务时，无论数据模式或任务类型如何，LLM-GTL都可以通过简单地指定提示符来促进直接推理。例如，如果目标是最优地利用先验知识对语义丰富的任务进行零概率推理，则可以使用T-lang模板来转换数据样本，然后将其输入LLM-GTL以生成输出。</p>
</li>
<li>在需要包含更多上下文示例并需要利用关于表格任务的元信息的情况下，t表模板成为最佳实践。</li>
<li>此外，即使在缺乏元信息的情况下，LLM-GTL仍然可以通过t -匿名模板提供预测，依靠在GTL过程中获得的固有统计学习。</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        
        
    </div>
    <a href="/2024/07/14/From-Supervised-to-Generative-A-Novel-Paradigm-for-Tabular-Deep-Learning-with-Large-Language-Models/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/07/14/DB-GPT-Empowering-Database-Interactions-with-Private-Large-Language-Models/">
        <h2 class="post-title">DB-GPT Empowering Database Interactions with Private Large Language Models</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/7/14
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p>[TOC]</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文研发了一个DB-GPT，它将llm与传统数据库系统集成在一起，以增强用户体验和可访问性。DB-GPT旨在<strong>理解自然语言查询</strong>，<strong>提供上下文感知响应</strong>，并以<strong>高精度生成复杂的SQL查询</strong>，使其成为从新手到专家的用户不可或缺的工具。DB-GPT的核心创新在于其私有LLMs技术，该技术对特定领域的语料库进行了<strong>微调</strong>，以维护用户隐私并确保数据安全，同时提供最先进的LLMs的优势。</p>
<p>DB-GPT的体系结构：</p>
<ul>
<li>一种新颖的<strong>检索增强生成(RAG)知识系统</strong></li>
<li>一种基于用户反馈不断提高性能的自适应学习机制</li>
<li>一种具有强大数据驱动代理的面向服务的多模型框架(SMMF)</li>
</ul>
<p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.17449">https://arxiv.org/pdf/2312.17449</a></p>
<p>论文代码：<a target="_blank" rel="noopener" href="https://github.com/eosphoros-ai/DB-GPT">https://github.com/eosphoros-ai/DB-GPT</a></p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><ul>
<li><p>如何使用llm授权数据库操作来构建强大的最终用户应用程序仍然是一个悬而未决的问题</p>
<ul>
<li>大多数现有作品采用了一种直接的方法——直接向常用的llm(如GPT-4)提供如何通过少量简短提示或上下文学习进行交互的说明<ul>
<li>优点：它不太可能过度拟合训练数据，并且易于适应新数据</li>
<li>缺点：与中等规模llm的微调替代方案相比，性能可能不是最优的</li>
</ul>
</li>
<li>此外，还有就是将llm驱动的自动推理和决策过程(又名代理)纳入数据库应用程序中。<ul>
<li>然而，知识代理通常是特定于任务的，而不是任务不可知的，这限制了它们的使用范围。</li>
<li>同时，虽然很重要，但对以llm为中心的数据库交互的隐私敏感设置的研究还不够。</li>
</ul>
</li>
</ul>
</li>
<li><p>本工作：研究出一个DB-GPT，用于llm增强应用程序的智能和生产就绪项目，可以使用私有化技术摄取、构建和访问数据。DB-GPT不仅利用llm固有的自然语言理解和生成能力，还通过代理和插件机制不断优化数据驱动引擎。</p>
</li>
<li><p>DB-GPT具有以下明显的<strong>优点</strong>:</p>
<ul>
<li>隐私和安全保护：DB-GPT允许用户<strong>部署在个人设备或本地服务器上</strong>，即使在没有Internet连接的场景下也可以运行。任何时候都不会有数据离开执行环境，完全消除了数据泄漏的风险。此外，代理去识别技术应用于数据处理模块，它作为中介，从数据集中模糊个人标识符，从而降低未经授权访问和利用私人信息的风险。</li>
<li><strong>多源知识库问答优化：</strong>DB-GPT构建了一个管道，将<strong>多源非结构化数据(PDF，网页，图像等)提取为中间表示</strong>，将其存储在结构化知识库中，检索最相关的部分，并在给定查询时生成全面的自然语言响应。该管道进行了效率优化，生成灵活，并接受双语查询。</li>
<li>文本到sql的微调：DB-GPT对<strong>几种常用的llm进行了微调</strong>，用于Text-toSQL任务。DB-GPT在与数据交互时显著降低了不需要SQL专业知识的用户的障碍。</li>
<li><strong>集成知识代理和插件：</strong>“代理”是一个自动推理和决策引擎。作为一个生产就绪的项目，DB-GPT支持具有高级数据分析的会话代理的开发和应用，其中这些自动化决策有助于数据上的交互式用例。它还提供了各种查询和检索服务插件，用作与数据交互的工具。</li>
</ul>
<h1 id="2-系统设计"><a href="#2-系统设计" class="headerlink" title="2 系统设计"></a>2 系统设计</h1><p>系统如下图所示：</p>
<img src="/2024/07/14/DB-GPT-Empowering-Database-Interactions-with-Private-Large-Language-Models/image-20240714153640078.png" class="" title="image-20240714153640078">
</li>
</ul>
<h2 id="2-1-用于QA的多源RAG（Retrieval-augmented-Generation）"><a href="#2-1-用于QA的多源RAG（Retrieval-augmented-Generation）" class="headerlink" title="2.1 用于QA的多源RAG（Retrieval-augmented Generation）"></a>2.1 用于QA的多源RAG（Retrieval-augmented Generation）</h2><p>构建一个多源知识库问答，它能够理解自然语言查询，提供上下文感知的回答，并且能够生成复杂且准确的SQL查询，从而提高从新手到专家各个层面用户的数据库交互体验。如图2所示，文章的RAG管道由三个阶段组成:知识构建、知识检索和自适应上下文学习(ICL)</p>
<img src="/2024/07/14/DB-GPT-Empowering-Database-Interactions-with-Private-Large-Language-Models/image-20240714153955130.png" class="" title="image-20240714153955130">
<ul>
<li>知识构建：这是知识检索的第一阶段，涉及将来自不同来源的文档（如PDF文件、网页等）转换为中间表示形式，并将它们存储在结构化的知识库中。<ul>
<li>知识库K是来自各种文档的集合，从$d_1^{loc},…,d_N^{loc}$，将每个文档$d_n$分为多个段落$p_{n,1}^{loc},…,p_{n,M_n}^{loc}$其中$M_n$表示第$n$个文档的段落索引，并通过一个编码器将$f_key$每个段落嵌入到多维嵌入$e_{n,m}^{loc}$​​中。然后，DB-GPT除了采用现有的基于向量的知识表示，还采用了倒排索引和图形索引技术，以便于准确地找到与上下文相关的数据</li>
</ul>
</li>
<li>知识检索：当用户提出一个语言查询时，<strong>系统会将查询嵌入到一个向量中，然后从知识库中检索与查询最相关的K个段落</strong>。检索方法可以基于余弦相似度或其他关键词匹配技术。<ul>
<li>当语言查询$x$出现时，它通过一个<strong>编码器$f_{query}$嵌入到向量</strong>中，然后我们从知识库当中<strong>检索前K个相关的段落</strong>，其中K是一个超参数。DG-GPT支持多种检索器模型，如Embeddingretrierver，它根据余弦相似度进行检索（$q^Te/||q|||e||$​​）Keywordretriiever，它匹配<strong>关键字</strong>而不是整个句子。</li>
</ul>
</li>
<li>学习嵌入和搜索：在这个阶段，系统通过训练编码器（encoders）来优化查询和文档段落的嵌入表示，使得相关性强的查询-段落对具有较高的相似度得分。<ul>
<li>自信地认为一个更高的相似度表示更相关的段落是因为训练了编码器$f_{key}$和$f_{query}$。他们在3.2进行优化了。直觉上，我们想要点积$q^Te$​对于查询段对来说是相对较大的，因为它们是相关的。编码器使用<strong>Multilingual-E5-base model</strong><ul>
<li>Multilingual-E5-base 是一种多语言嵌入模型，属于 Hugging Face 及其社区发布的语言模型系列之一。该模型基于跨语言嵌入技术，能够处理和生成多种语言的文本表示。</li>
</ul>
</li>
</ul>
</li>
<li><p>自适应ICL与LLM生成：在检索到相关段落后，系统使用自适应上下文学习方法来生成响应。这涉及到根据检索结果的相似度对它们进行排序，然后将最相关的几个结果插入到预定义的提示模板的上下文部分，最后由大型语言模型（LLM）生成回答。</p>
<ul>
<li>自适应上下文学习方法（Adaptive Context Learning，ACL）是一种动态调整和优化上下文内容以提高大语言模型（LLM）生成回答准确性和相关性的方法。在自适应上下文学习方法中，系统会根据检索到的相关段落，<strong>按照一定的策略和标准对其进行筛选和排序，将最相关的段落插入到预定义的提示模板中</strong>，从而为LLM提供最佳的上下文信息。</li>
<li><p>示例：</p>
<ul>
<li><p>假设有一个查询 “什么是机器学习？”，系统在知识库中检索到以下段落：</p>
<ul>
<li>段落A：机器学习是一种人工智能的分支，旨在通过经验和数据进行学习和改进。</li>
<li>段落B：深度学习是机器学习的一个子领域，它使用神经网络进行数据处理。</li>
<li>段落C：机器学习包括监督学习、无监督学习和强化学习等多种类型。</li>
</ul>
</li>
<li><p>经过排序和筛选，系统选择段落A和段落C作为最相关的上下文。然后将这些段落插入到<strong>提示模板</strong>中，如：</p>
<ul>
<li>问题：什么是机器学习？<br>上下文：<ol>
<li>机器学习是一种人工智能的分支，旨在通过经验和数据进行学习和改进。</li>
<li>机器学习包括监督学习、无监督学习和强化学习等多种类型。<br> 回答：</li>
</ol>
</li>
</ul>
</li>
<li><p>将上述提示模板输入LLM，由模型生成最终的回答，也就想要的SQL部分。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/2024/07/14/DB-GPT-Empowering-Database-Interactions-with-Private-Large-Language-Models/image-20240714160130046.png" class="" title="image-20240714160130046">
<h2 id="2-2-部署和推理-面向服务的多模型框架"><a href="#2-2-部署和推理-面向服务的多模型框架" class="headerlink" title="2.2 部署和推理:面向服务的多模型框架"></a>2.2 部署和推理:面向服务的多模型框架</h2><ul>
<li><p>在DB-GPT中，为了简化模型适配、提高模型部署效率和优化模型部署性能，提出了面向服务的多模型框架(SMMF)，提供了一种基于云的AI方法，使得开发者和企业能够访问预构建的、预训练的机器学习模型。这种方式简化了模型的获取和使用过程。</p>
</li>
<li><p>SMMF由两个主要部分组成</p>
<ul>
<li><strong>模型推理层</strong>，它被设计来适应各种大型语言模型（LLM）的推理平台，例如vLLM、HuggingFace Transformers、Text Generation Inference (TGI)和TensorRT等。</li>
<li><strong>模型部署层</strong>：作为底层推理层和上层模型服务功能之间的中介。它负责管理模型的元数据，并作为广泛部署架构的核心。<ul>
<li><strong>API服务器和模型处理器</strong>：在模型部署框架层中，API服务器和模型处理器提供强大的模型服务功能，供应用层使用。</li>
</ul>
</li>
<li><strong>模型控制器</strong>：模型控制器负责管理模型的元数据，并作为部署架构的中心节点。</li>
<li><strong>模型工作器</strong>：模型工作器与推理设备和基础设置建立直接连接，确保实施模型的高效性能。</li>
</ul>
</li>
</ul>
<h2 id="2-3-多代理的策略"><a href="#2-3-多代理的策略" class="headerlink" title="2.3 多代理的策略"></a>2.3 多代理的策略</h2><p>DB-GPT系统中的多代理策略旨在通过智能代理的协作和专业化，提高数据库操作的效率、灵活性和用户体验，同时保持系统的可扩展性和适应性。</p>
<p>DB-GPT为不同的数据库操作角色（如数据分析师、软件工程师和数据库架构师）分配<strong>特定的代理</strong>，每个代理都有其独特的功能和专长。通过一个协调机制，系统能够实现不同代理之间的协作，使它们能够相互通信、共享信息，并进行集体推理。</p>
<ul>
<li>就是我数据分析师，给的就是一个专门用来进行数据分析的DB-GPT</li>
</ul>
<p>DB-GPT基于Text-to-SQL调优的LLM，<strong>实现了具有高级数据库交互能力的代理的开发和应用</strong>。此外，与LlamaIndex的组件为特定用例提供更明确、约束的行为不同，DB-GPT赋予agent更强的一般推理能力，约束更少。</p>
<ul>
<li>LlamaIndex 是一个工具，用于连接大语言模型（LLM）和外部数据源，使 LLM 能够高效地访问和利用这些数据源中的信息进行任务处理。LlamaIndex 的主要功能包括数据索引、查询处理和信息检索，旨在增强 LLM 的回答能力和准确性。</li>
</ul>
<h2 id="2-4-DB插件"><a href="#2-4-DB插件" class="headerlink" title="2.4 DB插件"></a>2.4 DB插件</h2><p>DB-GPT系统中的DB插件旨在通过提供自然语言查询接口、增强LLMs的查询理解与执行能力、集成第三方服务以及支持端到端的数据分析流程，来增强系统与数据库的交互能力。这些插件允许系统更智能地解析和响应用户查询，执行SQL语句，并与外部数据源协同工作，从而提升数据库操作的效率和用户体验，同时保持系统的灵活性和可扩展性。</p>
<h1 id="3-模型和训练"><a href="#3-模型和训练" class="headerlink" title="3 模型和训练"></a>3 模型和训练</h1><h2 id="3-1-Text-to-SQL-微调"><a href="#3-1-Text-to-SQL-微调" class="headerlink" title="3.1 Text-to-SQL 微调"></a>3.1 Text-to-SQL 微调</h2><ul>
<li><p>是DB-GPT系统中使LLMs更加精准和高效地理解和生成SQL查询的关键步骤，它通过专门针对数据库查询语言的优化，提升了整个系统的数据库交互能力。</p>
</li>
<li><p>模型结构：我们从预训练的Qwen开始，该Qwen使用广泛的英语和汉语语料库进行预训练</p>
<ul>
<li>Qwen：通义千问，阿里巴巴的大语言模型</li>
</ul>
</li>
<li>数据集和训练：设计了一个特殊的模块DB-GPT- hub，它封装了预处理记录、模型加载和微调的管道。我们在Spider上微调Qwen训练分割，输入包括数据库描述和自然问题(见清单2)，输出是目标SQL。<ul>
<li>Spider 数据集是一个广泛用于研究和评估 Text-to-SQL 模型的基准数据集。该数据集由耶鲁大学的研究团队创建，旨在推动自然语言处理领域中将自然语言查询转换为 SQL 查询的研究和进展。<ul>
<li><strong>多样性</strong>：数据集中包含了来自不同领域的多个数据库，涵盖广泛的主题和结构。每个数据库都有不同的表和关系，提供了丰富多样的查询场景。</li>
<li><strong>复杂性</strong>：Spider 数据集中的查询从简单到复杂不等，涵盖了基础的单表查询、复杂的多表联接查询、嵌套查询等。这使得它对模型的语义理解和生成能力提出了较高的要求。</li>
<li><strong>语义丰富</strong>：自然语言查询包含丰富的语义信息，需要模型能够正确理解查询意图和细节。</li>
<li><strong>SQL 多样性</strong>：数据集中的 SQL 查询语句多样，涵盖了不同类型的 SQL 操作和函数，要求模型具备生成多种 SQL 结构的能力。</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/2024/07/14/DB-GPT-Empowering-Database-Interactions-with-Private-Large-Language-Models/image-20240714200312684.png" class="" title="image-20240714200312684">
<ul>
<li>关于文本到sql的微调llm的架构和评估的全部细节可以在我们即将发布的另一篇论文中找到。</li>
</ul>
<h2 id="3-2-Encoder-in-RAG"><a href="#3-2-Encoder-in-RAG" class="headerlink" title="3.2 Encoder in RAG"></a>3.2 Encoder in RAG</h2><p>Encoder in RAG是实现高效、准确问答功能的基础，它通过将自然语言查询转换为向量表示，并与知识库中的文档进行匹配，从而检索出最相关的信息，为生成准确的回答提供支持。</p>
<ul>
<li>模型架构：键和查询编码器$f_{key}$和$f_{query}$被初始化为Multilingual-E5-base模型结构因为我们支持双语的应用<ul>
<li>他们优化的目标函数如下：</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\ell=\mathbf{q}^\top\mathbf{e}_0-\log\sum_{i=0}^I\exp\left(\mathbf{q}^\top\mathbf{e}_i\right),</script><ul>
<li><p>其中，$e_0$是已知包含查询相关信息的段落的嵌入，以及其他$I$个嵌入$e_1,…,e_I$属于一组消极段落（模块3.2有说明他们是如何被选择的）。通过优化这个灯饰，对实际相关的查询段落对，点积$q^T$和$e$​​的值会变大</p>
<ul>
<li>这些消极段落是在训练RAG模型的编码器时使用的，用于区分和优化模型对相关段落的选择能力，下面有提到<strong>从整个段落池中随机抽取5个消极的响应</strong>的选取方法</li>
</ul>
</li>
<li><p>数据集和训练：我们使用查询-段落对来训练键和查询编码器，$f_{key}$和$f_{query}$​​，这些对是从DatabaseQA中收集的，我们对1000个查询-响应对进行采样，作为积极的对，对于每一个对，我们<strong>从整个段落池中随机抽取5个消极的响应</strong>。最后，我们收集了1000对查询-响应对用于训练和评估。然后将选择的配对分成700对训练配对、100对发展配对和200对测试配对。</p>
<ul>
<li><p>大概的代码流程：</p>
<ul>
<li><p>假设我们有以下查询-响应数据集</p>
<p>假设我们有一个包含1000个查询-响应对的初始数据集：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;query&#x27;</span>: [<span class="string">f&#x27;query_<span class="subst">&#123;i&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">1001</span>)],</span><br><span class="line">    <span class="string">&#x27;response&#x27;</span>: [<span class="string">f&#x27;response_<span class="subst">&#123;i&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">1001</span>)]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建DataFrame</span></span><br><span class="line">df = pd.DataFrame(data)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>采样消极响应</p>
<p>接下来，我们为每个查询-响应对随机采样5个消极响应：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采样消极响应</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample_negative_responses</span>(<span class="params">df, num_negatives=<span class="number">5</span></span>):</span><br><span class="line">    queries = df[<span class="string">&#x27;query&#x27;</span>].tolist()</span><br><span class="line">    responses = df[<span class="string">&#x27;response&#x27;</span>].tolist()</span><br><span class="line">    negative_samples = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> query, response <span class="keyword">in</span> <span class="built_in">zip</span>(queries, responses):</span><br><span class="line">        <span class="comment"># 从所有响应中排除正确的响应</span></span><br><span class="line">        negative_pool = [resp <span class="keyword">for</span> resp <span class="keyword">in</span> responses <span class="keyword">if</span> resp != response]</span><br><span class="line">        <span class="comment"># 随机抽取5个消极响应</span></span><br><span class="line">        negative_responses = random.sample(negative_pool, num_negatives)</span><br><span class="line">        <span class="keyword">for</span> neg_response <span class="keyword">in</span> negative_responses:</span><br><span class="line">            negative_samples.append(&#123;<span class="string">&#x27;query&#x27;</span>: query, <span class="string">&#x27;response&#x27;</span>: neg_response, <span class="string">&#x27;label&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> negative_samples</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取消极响应对</span></span><br><span class="line">negative_samples = sample_negative_responses(df)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>创建训练、发展和测试数据集</p>
<p>我们将原始的1000个积极对和生成的5000个消极对混合在一起，并将它们分成训练、发展和测试数据集：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建积极响应对并标注</span></span><br><span class="line">positive_samples = [&#123;<span class="string">&#x27;query&#x27;</span>: row[<span class="string">&#x27;query&#x27;</span>], <span class="string">&#x27;response&#x27;</span>: row[<span class="string">&#x27;response&#x27;</span>], <span class="string">&#x27;label&#x27;</span>: <span class="number">1</span>&#125; <span class="keyword">for</span> _, row <span class="keyword">in</span> df.iterrows()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并积极和消极响应对</span></span><br><span class="line">all_samples = positive_samples + negative_samples</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打乱所有样本</span></span><br><span class="line">random.shuffle(all_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建DataFrame</span></span><br><span class="line">all_samples_df = pd.DataFrame(all_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分数据集</span></span><br><span class="line">train_size = <span class="number">700</span></span><br><span class="line">dev_size = <span class="number">100</span></span><br><span class="line">test_size = <span class="number">200</span></span><br><span class="line"></span><br><span class="line">train_df = all_samples_df.iloc[:train_size]</span><br><span class="line">dev_df = all_samples_df.iloc[train_size:train_size + dev_size]</span><br><span class="line">test_df = all_samples_df.iloc[train_size + dev_size:train_size + dev_size + test_size]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;训练集大小: <span class="subst">&#123;<span class="built_in">len</span>(train_df)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;发展集大小: <span class="subst">&#123;<span class="built_in">len</span>(dev_df)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;测试集大小: <span class="subst">&#123;<span class="built_in">len</span>(test_df)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>我们将查询-响应对传递给模型，以产生每个对的标量分数，并最大化正对的分数，同时通过交叉熵损失最小化负对的分数。</p>
</li>
</ul>
<h2 id="3-3-应用和部署细节"><a href="#3-3-应用和部署细节" class="headerlink" title="3.3 应用和部署细节"></a>3.3 应用和部署细节</h2><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h1><p>提出了旨在评估DB-GPT系统性能的实验，包括文本到sql响应的生成质量(第3.1节)、我们提出的RAG机制的QA性能(第2.1节)和SMMF的效率性能(?)。我们还提供了生成数据分析的定性结果(第2.4节)。</p>
<h2 id="4-1-Text-to-SQL-Evalution"><a href="#4-1-Text-to-SQL-Evalution" class="headerlink" title="4.1 Text-to-SQL Evalution"></a>4.1 Text-to-SQL Evalution</h2><ul>
<li><p><strong>数据集：</strong>Spider数据集</p>
</li>
<li><p><strong>指标：</strong>执行精度(EX)作为度量标准</p>
<ul>
<li>EX在一些数据库实例上将预测的SQL查询的执行输出与实际的SQL查询的执行输出进行比较。EX越高越好</li>
<li>步骤<ul>
<li><strong>执行SQL查询</strong>：将模型生成的SQL查询和真实（或基准）SQL查询分别在数据库上执行。</li>
<li><strong>获取结果集</strong>：记录两个查询的执行结果，通常是从数据库检索到的数据行或记录集。</li>
<li><strong>比较结果集</strong>：比较两个结果集，确定它们是否相同或在某种程度上相似。</li>
<li><strong>计算相似度</strong>：使用某种形式的相似度度量（例如，Jaccard相似度、精确匹配、F1分数等）来量化两个结果集之间的相似性。</li>
<li><strong>确定阈值</strong>：根据任务的具体要求，确定一个阈值来判断何时认为两个结果集是足够相似的。</li>
<li><strong>生成执行精度分数</strong>：基于比较结果和相似度度量，生成一个0到1之间的分数，表示模型生成的SQL查询的执行精度。在某些情况下，完全一致的结果集可能会得到1.0的分数，而完全不匹配的结果集可能会得到0.0的分数。</li>
<li><strong>平均执行精度</strong>：如果是在多个查询上评估模型，可能会计算所有查询的平均执行精度，以得到模型整体性能的度量。</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/2024/07/14/DB-GPT-Empowering-Database-Interactions-with-Private-Large-Language-Models/image-20240714202654325.png" class="" title="image-20240714202654325">
<h2 id="4-2-RAG-Evaluation"><a href="#4-2-RAG-Evaluation" class="headerlink" title="4.2 RAG Evaluation"></a>4.2 RAG Evaluation</h2><ul>
<li><strong>数据集</strong>：构建了两个QA数据集:DatabaseQA和FinancialQA。<ul>
<li>对于DatabaseQA，我们从三个代表性数据库系统中收集了1000个pdf格式的公共教程:OceanBase、MySQL、MongoDB</li>
<li>对于FinancialQA，我们从研究机构发表的1000份文件中取样。对于每个数据集，我们构建了100个问题用于测试，其中的问题由专家标注了难度。</li>
</ul>
</li>
<li><strong>指标：</strong>有三位专家对每个回答进行评分，评分从0到5，得分越高的回答越好，并将他们的平均值作为最终得分。</li>
</ul>
<img src="/2024/07/14/DB-GPT-Empowering-Database-Interactions-with-Private-Large-Language-Models/image-20240714203229767.png" class="" title="image-20240714203229767">
<h2 id="4-3-SMMF-Evaluation"><a href="#4-3-SMMF-Evaluation" class="headerlink" title="4.3 SMMF Evaluation"></a>4.3 SMMF Evaluation</h2><ul>
<li><p><strong>数据集：</strong>测试在一台服务器上执行，在所有实验中，我们使用相同的提示符，将8个标记作为输入，同时将输出长度设置为256个标记。</p>
</li>
<li><p><strong>指标</strong>：</p>
<ul>
<li>第一个令牌延迟(FTL))：以毫秒为单位测量，它表示从DB-GPT模型部署框架接收请求到推理框架解码第一个令牌所花费的时间。</li>
<li>推断延迟(IL):以秒为单位测量，它表示从DBGPT模型部署框架接收请求到推断框架解码完整响应所花费的时间。</li>
<li>吞吐量:DB-GPT模型部署框架在所有请求中每秒处理的令牌总数。</li>
</ul>
</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/LLMs-SQL/" style="color: #03a9f4">LLMs SQL</a>
        </span>
        
    </div>
    <a href="/2024/07/14/DB-GPT-Empowering-Database-Interactions-with-Private-Large-Language-Models/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/07/14/CHATDB-AUGMENTING-LLMS-WITH-DATABASES-AS-THEIR-SYMBOLIC-MEMORY/">
        <h2 class="post-title">CHATDB-AUGMENTING-LLMsS-WITH-DATABASES-AS-THEIR-SYMBOLIC-MEMORY</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/7/14
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>具有内存的大型语言模型(LLMs)在计算上是通用的，然而，主流LLMs并没有<strong>充分利用内存</strong>，而且设计受到生物大脑的严重影响。传统的神经记忆机制由于其<strong>近似性质</strong>和<strong>容易累积错误</strong>，无法支持LLMs<strong>模拟复杂推理</strong>。在本文中，我们从现代计算机体系结构中寻求灵感，以<strong>增强具有符号记忆的LLMs</strong>，用于复杂的<strong>多跳推理</strong>。这样一个符号内存框架被实例化为一个LLMs和一组SQL数据库，其中LLMs生成SQL指令来操作SQL数据库。</p>
<ul>
<li>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.03901">https://arxiv.org/abs/2306.03901</a></li>
<li>代码链接：<a target="_blank" rel="noopener" href="https://github.com/huchenxucs/ChatDB">https://github.com/huchenxucs/ChatDB</a></li>
</ul>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1><p>LLMs存在以下局限性：</p>
<ul>
<li><p>与语言模型的多回合交互会产生大量的令牌，这很容易<strong>超过llm的输入令牌限制</strong>。</p>
<ul>
<li>令牌：代表模型可以理解和生成的最小意义单位，是LLM 进行处理的最小单元。在文本的背景下，一个Token可以是一个单词、单词的一部分（子词）或甚至是一个字符，这取决于Token化过程。</li>
<li>随着交互的进行，llm必须维护上下文信息(例如，用户输入和先前的响应)，并根据累积的数据生成响应。然而，简单地将所有上下文信息连接起来并将其塞进llm很容易超出llm的处理能力并积累错误，从而导致模型失去对对话的跟踪并产生不太准确的响应。</li>
</ul>
</li>
<li><p>一些神经记忆机制已被探索已克服llm的token输入有限问题。<strong>记忆组件</strong>作为存储和检索系统，从以前的交互中获取相关信息。然而，用传统的神经记忆增强llm通常会导致<strong>存储、检索和操作内存中的历史信息的困难</strong>，特别是对于需要复杂的多跳推理的任务。两个主要原因是:</p>
<ul>
<li>它们<strong>没有以结构化的形式存储历史信息</strong>;</li>
<li>它们对存储在存储器中的信息的操作<strong>不是象征性的</strong>，因为它们都依赖于一些向量相似性计算，这些计算可能是不准确的，从而导致错误的积累。</li>
</ul>
</li>
<li><p>解决方法：</p>
<ul>
<li>使用SQL语言作为llm的<strong>新型符号内存</strong>。整个框架命名为ChatDB。结构如下图所示：</li>
</ul>
<img src="/2024/07/14/CHATDB-AUGMENTING-LLMS-WITH-DATABASES-AS-THEIR-SYMBOLIC-MEMORY/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-07-14%20102945.jpg" class="" title="屏幕截图 2024-07-14 102945">
<p>图1:ChatDB的总体工作流程。LLM控制器控制对存储器的读写操作。存储器存储历史信息并提供相关的历史信息，以帮助响应用户输入。在ChatDB中，我们专注于用数据库作为符号内存来增强llm</p>
<ul>
<li><p>具体组成</p>
<ul>
<li><p>LLM控制器：LLM控制器可以是任何常用的LLM </p>
</li>
<li><p>LLM的内存可以是符号或非符号，也可以是两者的组合，它负责存储历史信息，并在需要时提供信息，以帮助LLM响应用户输入</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>在ChatDB中，我们着重于<strong>使用数据库作为符号内存</strong>，它允许通过<strong>执行符号语言(即SQL语句)来结构化地存储历史信息</strong>。这些SQL语句是<strong>由LLM生成的</strong>。在需要精确记录、修改、查询、删除和分析历史数据的场景中，将数据库合并为符号内存特别有用。例如，商店经理需要维护日常销售记录，而使用纯文本或矩阵作为内存是不合适的</p>
</li>
<li><p>然而，使用数<strong>据库作为外部符号内存</strong>是非常合适的。数据库可以使用SQL语句进行精确的操作，包括数据的插入、删除、更新和选择。因此，使用数据库作为外部符号存储器确保了管理和操作历史数据的准确性和效率，显著提高了llm在需要高精度和长期数据记录和处理的场景中的性能。</p>
<ul>
<li><strong>符号内存</strong>是指大脑或人工智能系统中存储和处理符号信息的能力。<strong>就是利用SQL去处理数据</strong></li>
<li><strong>外部符号内存</strong>是指将符号信息存储在外部设备或介质上，而不是仅限于系统内部存储。<strong>是数据库本身，用来存储数据的地方</strong></li>
</ul>
</li>
<li><p>在ChatDB框架中，我们提出了<strong>内存链(CoM)</strong>方法来更有效地操作外部符号内存，从而进一步增强llm的推理能力。<strong>内存链方法</strong>将<strong>用户输入转换为一系列导致最终结果的中间内存操作步骤</strong>。通过内存链方法，<strong>将一个复杂的问题分解为多个记忆操作步骤</strong>，大大降低了问题解决的复杂性。在ChatDB中，每个中间步骤都涉及一个或多个SQL语句。</p>
</li>
<li><p><strong>总结</strong>：首先，建议<strong>用数据库作为llm的外部符号内存</strong>，允许历史数据的结构化存储，并允许使用SQL语句进行符号和复杂的数据操作。其次，我们的<strong>内存链方法</strong>通过将用户输入转换为多步中间内存操作来实现有效的内存操作，这增强了ChatDB的性能，使其能够以更高的准确性和稳定性处理复杂的多表数据库交互。</p>
</li>
</ul>
<h1 id="3-ChatDB"><a href="#3-ChatDB" class="headerlink" title="3. ChatDB"></a>3. ChatDB</h1><h2 id="3-1-任务定义"><a href="#3-1-任务定义" class="headerlink" title="3.1 任务定义"></a>3.1 任务定义</h2><ul>
<li>给定用户以自然语言输入和数据库中现有表的详细信息(如果没有现有表，则不需要)，目标是操作符号内存，即外部数据库，以满足用户的请求。<ul>
<li>例如，如果用户(例如，商店经理)命令是记录、修改、查询和删除特定的数据，那么相应的SQL操作应该是分别在适当的表中插入、更新、选择和删除相关数据。这些操作通常涉及数据库中的多个表</li>
</ul>
</li>
</ul>
<h2 id="3-2框架概述"><a href="#3-2框架概述" class="headerlink" title="3.2框架概述"></a>3.2框架概述</h2><p>ChatDB框架由三个主要阶段组成：<strong>输入处理</strong>、<strong>记忆链</strong>和<strong>响应总结</strong></p>
<img src="/2024/07/14/CHATDB-AUGMENTING-LLMS-WITH-DATABASES-AS-THEIR-SYMBOLIC-MEMORY/image-20240715224432295.png" class="" title="image-20240715224432295">
<ul>
<li><strong>输入处理：</strong>如果响应用户输入需要使用内存，ChatDB通过<strong>llm生成一系列中间步骤来操作符号内存</strong>。否则，我们直接使用llm来生成回复。</li>
<li><strong>Chain-of-Memory：</strong>ChatDB执行一系列中间内存操作步骤来与符号内存交互。ChatDB根据<strong>先前生成的一系列SQL语句依次操作符号内存</strong>，包括插入、更新、选择、删除等操作。外部数据库执行相应的SQL语句，更新数据库，并返回结果。值得注意的是，ChatDB在执行内存操作步骤之前，<strong>会根据前面SQL语句的结果来决定是否更新内存操作步骤</strong>。ChatDB按照相同的过程执行下一步，直到完成对内存的所有操作。</li>
<li><strong>响应总结：</strong>ChatDB根据一系列<strong>内存链步骤的结果总结对用户的最终响应</strong></li>
</ul>
<h2 id="3-3内存链"><a href="#3-3内存链" class="headerlink" title="3.3内存链"></a>3.3内存链</h2><p>思维链强调将复杂推理分解为一系列中间步骤。</p>
<p>通过提供符号内存机制来支持与这些中间步骤相关联的存储，可以将内存链(CoM)视为一种增强思维链的方法。</p>
<p>内存链的目的是提高llm在操作符号内存时的推理能力和鲁棒性。该方法包括<strong>将用户输入转换为中间内存操作序列</strong>，使llm能够<strong>以符号方式更准确有效地操作内存</strong>。对于涉及与历史数据进行复杂而准确交互的实际应用程序(例如管理设置中的记录保存和数据分析)，操作符号内存的能力特别有价值。</p>
<p>为了提高我们方法的性能和稳定性，我们采用了<strong>上下文学习</strong>，提供了几个<strong>内存链步骤序列</strong>和<strong>思维链提示</strong>的提示示例。一个强大而准确的<strong>内存链</strong>过程使llm能够更好地对符号记忆进行推理，并处理更复杂的场景。</p>
<ul>
<li><p><strong>上下文学习</strong>是指模型在给定的上下文环境中学习和推理的能力。通过提供额外的上下文信息，模型能够更准确地理解输入内容并作出相应的反应。</p>
</li>
<li><p><strong>思维链提示</strong>是指在模型推理过程中，通过提供明确的思维过程提示，引导模型逐步进行复杂推理。这种方法能够帮助模型在多步骤推理中保持逻辑一致性和准确性。</p>
<ul>
<li><p><strong>示例：</strong> 假设我们有一个数学问题求解系统，帮助学生解决复杂的数学问题。</p>
<p><strong>问题：</strong> “如果一个数的三倍加上五等于二十，求这个数。”</p>
<p><strong>思维链提示：</strong></p>
<ol>
<li>设这个数为 xxx。</li>
<li>写出等式 3x+5=203x + 5 = 203x+5=20。</li>
<li>解方程 3x=20−53x = 20 - 53x=20−5。</li>
<li>解得 3x=153x = 153x=15。</li>
<li>求得 x=5x = 5x=5。</li>
</ol>
</li>
</ul>
</li>
</ul>
<p>内存链的优点是双重的。</p>
<ul>
<li>首先，它使llm能够以更高的精度执行复杂的数据库操作，增强了它们在符号内存上的多跳推理能力。</li>
<li>其次，通过将复杂操作分解为一系列中间内存操作，内存链方法增强了llm处理复杂多表交互的能力。</li>
</ul>
<p>这种方法使LLMs能够更好地处理边缘情况和意外情况，使其成为现实应用程序中很有前途的方法。</p>
<img src="/2024/07/14/CHATDB-AUGMENTING-LLMS-WITH-DATABASES-AS-THEIR-SYMBOLIC-MEMORY/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-07-14%20115317.jpg" class="" title="屏幕截图 2024-07-14 115317">
<h2 id="3-4与以往记忆增强llm的比较"><a href="#3-4与以往记忆增强llm的比较" class="headerlink" title="3.4与以往记忆增强llm的比较"></a>3.4与以往记忆增强llm的比较</h2><img src="/2024/07/14/CHATDB-AUGMENTING-LLMS-WITH-DATABASES-AS-THEIR-SYMBOLIC-MEMORY/image-20240714120401054.png" class="" title="image-20240714120401054">
<h1 id="4-Evaluation"><a href="#4-Evaluation" class="headerlink" title="4 Evaluation"></a>4 Evaluation</h1><h2 id="4-1实验设置"><a href="#4-1实验设置" class="headerlink" title="4.1实验设置"></a>4.1实验设置</h2><p>构建了一个模拟水果店管理的合成数据集。</p>
<p>为了评估模型的性能，我们收集了一组50个带有注释标准答案的问题。这些问题的难度各不相同，从需要多跳推理的困难问题到只需要从历史数据中检索信息的简单问题。有15个简单问题和35个难题。每个问题都由模型独立回答。</p>
<h3 id="4-1-1-模型配置"><a href="#4-1-1-模型配置" class="headerlink" title="4.1.1 模型配置"></a>4.1.1 模型配置</h3><p>ChatDB：使用的LLM为ChatGPT (GPT-3.5 Turbo)，超参数temperature设置为0。我们使用MySQL数据库作为外部符号内存。</p>
<p>基线：我们使用ChatGPT (GPT-3.5 Turbo)作为基准模型，最大令牌长度为4096。与ChatDB类似，我们将温度设置为0。</p>
<h3 id="4-1-2-数据集"><a href="#4-1-2-数据集" class="headerlink" title="4.1.2 数据集"></a>4.1.2 数据集</h3><ul>
<li>合成了一个水果店管理记录的数据集，称为“水果店数据集”。该数据集模拟了商店中的四种常见操作:购买、销售、更改价格和退货。我们确保所有历史记录都是有效的，不会遇到负面库存等问题。我们按时间顺序生成70条记录，总共约3.3万个令牌，这在ChatGPT的最大令牌长度限制(4096个令牌)之内。<ul>
<li>为什么要限制数据集的令牌长度?如果数据集的令牌长度超过ChatGPT的最大令牌长度，则需要内存。然而，主流的基于向量嵌入的记忆检索方法容易出错。这不可避免地会导致ChatGPT性能的下降，这是不希望的。</li>
<li>因此，我们故意将数据集的令牌长度设计在ChatGPT的最大令牌长度范围内，以避免使用内存并最大化模型的性能。请注意，ChatDB的性能通常不受数据集令牌长度的影响。因此，如果ChatDB在数据集较小时优于ChatGPT，则表明当数据集较大时，ChatDB也优于内存增强的ChatGPT。</li>
</ul>
</li>
</ul>
<h3 id="4-1-3处理记录"><a href="#4-1-3处理记录" class="headerlink" title="4.1.3处理记录"></a>4.1.3处理记录</h3><ul>
<li>对于ChatDB，第一步是初始化数据库。我们需要为特定的任务场景生成合理的数据库模式，并在数据库中创建表。数据库模式的生成可以手工完成，也可以使用llm完成。</li>
<li>接下来，对于数据集中的每条记录，ChatDB逐个处理它们。使用LLM控制器，ChatDB按照算法1操作外部数据库(即符号内存)。我们提供了ChatDB对Fruit Shop Dataset中的四种常见操作(即购买、销售、更改价格和退货)的响应示例<ul>
<li>值得强调的是，ChatDB是一条一条地处理记录，因此它对记录总数不敏感。此外，ChatDB中数据库操作的每一步都是象征性的，没有错误。因此，理论上，ChatDB可以处理无限数量的历史记录，而不会牺牲性能。然而，对于ChatGPT或现有的内存增强llm，过长的历史记录会显著降低性能。在这个实验中，对于ChatGPT基线，由于记录不长，我们简单地将它们视为提示的一部分。</li>
</ul>
</li>
</ul>
<h3 id="4-1-4-回答问题"><a href="#4-1-4-回答问题" class="headerlink" title="4.1.4 回答问题"></a>4.1.4 回答问题</h3><p>ChatDB在回答问题时不再需要记录作为提示符的一部分。处理完记录后，信息被存储在符号存储器中。在算法1之后，ChatDB利用SQL语句执行一系列数据库查询(包括计算)来回答问题。另一方面，ChatGPT将记录作为提示的一部分，并直接提出问题。</p>
<h2 id="4-2结果"><a href="#4-2结果" class="headerlink" title="4.2结果"></a>4.2结果</h2><p>ChatDB的准确率明显高于ChatGPT。虽然ChatGPT能够回答简单的问题，但它在处理需要多跳推理和精确计算的困难问题时却有所欠缺。因此，ChatGPT对这些难题的准确率很低。相比之下，ChatDB显示出非常高的准确率，强调了将数据库用作符号内存的优势。该方法不仅防止了误差积累，而且提高了llm的多跳推理和精确计算能力。</p>
<p>ChatDB的优势体现在两个方面</p>
<ul>
<li>通过记忆链方法，将复杂的问题分解为多个步骤的存储操作，简化了问题的复杂性。每一步的结果都被准确地存储为中间结果，并用于后续步骤，这极大地帮助了复杂的推理。</li>
<li>符号存储器可以实现精确的操作和计算。ChatDB通过执行SQL语句，将许多计算任务委托给外部数据库，保证了每一步的准确性，防止了错误的积累。</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/CoM-LLMs-SQL/" style="color: #ff7d73">CoM LLMs SQL</a>
        </span>
        
    </div>
    <a href="/2024/07/14/CHATDB-AUGMENTING-LLMS-WITH-DATABASES-AS-THEIR-SYMBOLIC-MEMORY/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/07/13/Improving-Demonstration-Diversity-by-Human-Free-Fusing-for-Text-to-SQL/">
        <h2 class="post-title">Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/7/13
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p>[TOC]</p>
<h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1.Abstract"></a>1.Abstract</h1><ul>
<li><p>使用<strong>大型语言模型(llm)的上下文学习</strong>是当前<strong>Text-to-SQL​</strong>的主流方法。先前的研究已经探索了从人工标记的示范池中选择相关的示范，但这些方法缺乏<strong>多样性</strong>，并且需要高昂的标记成本。</p>
</li>
<li><p>在这项工作中，我们将讨论度量和增强文本到sql演示池的多样性。首先，我们引入了一个<strong>多样性度量</strong>，并提出了现有标签数据的多样性可以进一步增强。受这些发现的启发，提出了<strong>fuse</strong>，迭代地融合演示，以创建基于人类标记的多样化演示池，甚至从头开始使用llm，从而降低标记成本。FUSED在现有标记的基础上实现了3.2%的平均改进，在几个主流数据集上实现了5.0%的从头开始改进，证明了它的有效性</p>
<ul>
<li>多样性指的是在文本到SQL（Text-to-SQL）任务中，用于指导语言模型生成SQL查询的示例集合在内容、结构和语义上的差异性和丰富性。<ol>
<li><strong>内容多样性</strong>：示例应涵盖不同的主题和信息点，确保模型能够处理各种类型的用户查询。</li>
<li><strong>结构多样性</strong>：示例应包括不同的SQL语句结构，如不同的查询子句（SELECT、WHERE、GROUP BY等）和操作符，以帮助模型理解并生成各种结构的SQL查询。</li>
<li><strong>语义多样性</strong>：示例应表达不同的语义意图和逻辑，反映用户查询的不同需求和目标。</li>
<li><strong>数据库多样性</strong>：示例应来源于不同的数据库和表结构，以增强模型对不同数据库环境的适应性。</li>
<li><strong>查询复杂性</strong>：示例应包括从简单到复杂的查询，使模型能够处理不同难度级别的SQL生成任务。</li>
</ol>
</li>
</ul>
<p>文章链接：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.10663">https://arxiv.org/pdf/2402.10663</a><br></p>
<p>代码链接：<a target="_blank" rel="noopener" href="https://github.com/zirui-HIT/Fused">https://github.com/zirui-HIT/Fused</a></p>
</li>
</ul>
<h1 id="2-Analysis"><a href="#2-Analysis" class="headerlink" title="2.Analysis"></a>2.Analysis</h1><p>提出可以进一步增强现有标记演示池的多样性。首先，我们讨论了示范池的高度多样性的必要性。然后，我们引入了一个度量来量化演示池的多样性。基于这一度量，我们讨论了现有标签数据的多样性可以进一步增强。我们将目前的度量标准与附录A中其他现有度量标准进行比较。</p>
<h2 id="Necessity-of-the-Diversity"><a href="#Necessity-of-the-Diversity" class="headerlink" title="Necessity of the Diversity"></a>Necessity of the Diversity</h2><ul>
<li>LLms通过模仿提供的示范来生成答案，也就是，给定一个用户问题，以前的工作从演示池中选择最相似的演示来指导LLMs生成答案，但是，由于用户问题是<strong>不可预测的</strong>，因此演示池应该尽可能多样化，以涵盖各种用户问题。多样性越高，任何用户问题与演示之间的相似性越高，从而更好地指导答案生成;多样性越低，越来越多的用户问题与演示不太相似，从而降低了模型的性能。</li>
</ul>
<h2 id="Diversity-Measurement"><a href="#Diversity-Measurement" class="headerlink" title="Diversity Measurement"></a>Diversity Measurement</h2><ul>
<li>基于前面的讨论，我们使用<strong>与演示池相似度最低</strong>的用户问题来度量演示池的多样性。</li>
<li>形式上$D=\{d_i\}$表示演示池，$U=\{u\}$表示用户问题，$sim(U,D)$表示U和D之间的相似度，以本文中它们编码向量之间的欧氏距离的倒数计算，利用下面公式来衡量示范池$D$​的多样性，称为多样性度量，该指标对应于与演示池中最相似的演示相似度最小的用户问题与任何其他用户问题的相似度。</li>
</ul>
<p><script type="math/tex">\mathrm{DM}=\min_{u\in U}\max_{d_i\in D}\mathrm{sim}(u,d_i)</script>​</p>
<h2 id="Diversity-of-the-Existing-Labeling-Can-be-Further-Enhanced"><a href="#Diversity-of-the-Existing-Labeling-Can-be-Further-Enhanced" class="headerlink" title="Diversity of the Existing Labeling Can be Further Enhanced"></a>Diversity of the Existing Labeling Can be Further Enhanced</h2><ul>
<li>使用上面的度量，我们然后度量现有文本到sql标记演示池的多样性。现有标注演示池的DM和性能如图4和图5所示。这些数字显示了DM和性能明显高于现有标记数据的其他演示池。因此，虽然现有的标签表现出较高的多样性，但可以进一步改进，从而提高性能。因此，我们接下来将讨论综合演示的方法，以增强演示池的多样性。</li>
</ul>
<div style="display: flex; justify-content: space-around;">
  <img src="/2024/07/13/Improving-Demonstration-Diversity-by-Human-Free-Fusing-for-Text-to-SQL/0_0.jpg"  style="width: 200px; height: auto; margin: 10px;">
  <img src="/2024/07/13/Improving-Demonstration-Diversity-by-Human-Free-Fusing-for-Text-to-SQL/undefined"  style="width: 200px; height: auto; margin: 10px;">
</div>

<h1 id="3-Method"><a href="#3-Method" class="headerlink" title="3.Method"></a>3.Method</h1><p>方法侧重于如何用llm合成给定数据库的新演示。考虑到仅依靠采样生成直接生成演示的多样性较差提出了通过迭代融合不同演示的方法进行合成，如图3所示。在每次迭代中，我们指导模型生成与之前迭代不相似的演示，从而增强多样性。我们从理论上证明了我们的方法可以增强附录D中的DM。</p>
<p><img src="C:\Users\s\AppData\Roaming\Typora\typora-user-images\image-20240713165722636.png" alt="label"></p>
<p>方法简单解释：首先基于SQL关键字(例如，WHERE, ORDER BY)对演示进行聚类。然后，我们对每个集群的演示进行采样和融合。融合的演示包含了不同于采样演示的WHERE和ORDER BY，从而增强了演示的多样性。在实践中，我们使用编码的用户问题而不是SQL关键字进行合成，因为用户问题比SQL具有更多的语义信息</p>
<h2 id="3-1整体预览"><a href="#3-1整体预览" class="headerlink" title="3.1整体预览"></a>3.1整体预览</h2>
            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/sql/" style="color: #00bcd4">sql</a>
        </span>
        
    </div>
    <a href="/2024/07/13/Improving-Demonstration-Diversity-by-Human-Free-Fusing-for-Text-to-SQL/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/07/12/DTS-SQL-Decomposed-Text-to-SQL-with-Small-Large-Language-Models/">
        <h2 class="post-title">DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/7/12
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1. Abstract"></a>1. Abstract</h1><p>文章介绍了一种名为DTS-SQL的新技术，它是一种分解式文本到SQL（Text-to-SQL）的方法，旨在减小开源小型语言模型（LLMs）与专有大型语言模型之间的性能差距。<br></p>
<p>文章链接：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.01117">https://arxiv.org/pdf/2402.01117</a><br>代码链接：<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DTS-SQL-2A42/README.md">https://anonymous.4open.science/r/DTS-SQL-2A42/README.md</a></p>
<h1 id="2-Methodology"><a href="#2-Methodology" class="headerlink" title="2. Methodology"></a>2. Methodology</h1><ul>
<li><p>LLMs的一个显著发展是，其会进行训练后细化，增强了它们与首选行为的一致性。<br></p>
<ul>
<li>训练后细化指的是大型语言模型（LLMs）完成初步的<strong>预训练阶段</strong>之后，进行的进一步训练或调整过程。这个阶段的目的是使模型更好地适应特定的应用场景或任务，提高其性能和输出结果的准确性。</li>
</ul>
</li>
<li><p>目前由于缺乏包含人类或者人工智能反馈的广泛数据集，导致人们导致人们主要关注<strong>Text-to-SQL</strong>的监督微调。这种方法需要一组特定的指令或提示及其相应的输出或响应。<br></p>
<ul>
<li>关于为什么要关注<strong>Text-to-SQL</strong>：Text-to-SQL系统能够将自然语言描述转化成对应的SQL查询语句，这项技术能够有效地辅助人们对海量的数据库进行查询。<ul>
<li>Text-to-SQL就是这样一项转化自然语言描述为SQL查询语句的技术。举个例子：当我们询问智能助手 “贾樟柯导演是在哪出生的啊？”，Text-to-SQL模型就会先根据问句解析出SQL语句“SELECT birth_city FROM director WHERE name =”贾樟柯””，再对数据库执行该命令，最后向用户返回查询结果“山西省汾阳市”。<br></li>
<li>《一文了解Text-to-SQL》文章链接：<a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2019-12-27-11">https://www.jiqizhixin.com/articles/2019-12-27-11</a></li>
</ul>
</li>
</ul>
</li>
<li>文章研究在Text-to-SQL上下文中对llm进行监督微调的既定方法。介绍了新的两步微调方法，旨在增强文本到sql领域中模型的性能</li>
</ul>
<h2 id="2-1有监督问题到SQL的微调（Supervised-fine-tunning-for-Text-to-SQL）"><a href="#2-1有监督问题到SQL的微调（Supervised-fine-tunning-for-Text-to-SQL）" class="headerlink" title="2.1有监督问题到SQL的微调（Supervised fine-tunning for Text-to-SQL）"></a>2.1有监督问题到SQL的微调（Supervised fine-tunning for Text-to-SQL）</h2><ul>
<li>在这个小节中，将探索在开源社区实践的Text-to-SQL任务的监督调优过程</li>
<li><p>给定一组数据库$D_i$ 其中包含对问题$q_i$和相应的SQL查询$s_i$，目标是使用一组训练数据$T=\{(q_i,s_i,D_i)\}$对大预言模型M进行微调，其中$q_i$和$s_i$表示自然语言问题和及其在数据库$D_i$上关联的SQL查询。监督微调的目标是为了<strong>最小化经验损失</strong>：<br>$\min_{\sigma, M} \frac{1}{|T|} \sum_{i=1}^{|T|} \mathcal{L}(M(\sigma_f(q_i, D_i, s_i)))$</p>
<ul>
<li>其中，$\mathcal{L}$是测量实际SQL查询和模型生成的SQL查询之间的差异。</li>
<li>$\sigma_f$​​决定了问题、数据库模式和SQL查询的格式</li>
<li>$M^<em>$表示经过微调（fine-tuning）后的模型参数，M表示model，然后</em>表示经过微调</li>
</ul>
</li>
<li><p>主要的问题就是，我们事先不知道数据库中的表和哪些表是和要生成的SQL相关</p>
</li>
<li>因此一种常用的调优方法包括在提示中包括<strong>所有表的模式</strong>以及<strong>问题</strong>和<strong>SQL对</strong>。<ul>
<li>这种方法有两个目的:<ul>
<li>教模型生成正确的SQL查询</li>
<li>并从所有提供的表中识别相关的表。</li>
</ul>
</li>
<li>这种同时针对两个目标进行训练的方法使llm的SQL生成任务变得复杂，特别是对于只有几十亿个参数的较小模型。每个任务——生成SQL查询并正确链接到相关模式——都需要自己的推理过程。在大型语言模型中，很大一部分错误可归因于不正确的模式链接，这是该领域的一个主要挑战</li>
</ul>
</li>
</ul>
<h2 id="2-2分解监督微调（Decomposed-Supervised-Fine-tuning）"><a href="#2-2分解监督微调（Decomposed-Supervised-Fine-tuning）" class="headerlink" title="2.2分解监督微调（Decomposed Supervised Fine-tuning）"></a>2.2分解监督微调（Decomposed Supervised Fine-tuning）</h2><p>提出了一个两阶段的微调过程，将模式链接和SQL生成分开，旨在提高NL-to-SQL系统的性能</p>
<h3 id="2-2-1模式链接微调-（-Schema-linking-Fine-tuning）"><a href="#2-2-1模式链接微调-（-Schema-linking-Fine-tuning）" class="headerlink" title="2.2.1模式链接微调 （ Schema-linking Fine-tuning）"></a>2.2.1模式链接微调 （ Schema-linking Fine-tuning）</h3><ul>
<li>模式链接涉及识别数据库中相关的列和表，以响应自然语言查询。<ul>
<li>它已被证明可以增强跨域泛化性并促进复杂查询的创建</li>
<li>在之前的研究中，模式链接主要是通过上下文学习方法或在SQL生成的微调过程中隐含地完成的</li>
</ul>
</li>
<li>在本文章当中，将模式链接视为一项独特的任务，并显式微调llm，以便在使用自然语言查询时识别相关的表和列。</li>
<li>给定训练数据集$T=\{(q_i,s_i,D_i)\}$，提取 SQL查询中所使用的所有列和表，创建一个新的数据集$T=\{(q_i,T_i,C_i,D_i)\}$，其中$T_i$和$C_i$表示SQL查询$s_i$中使用的表和列表。在模式链接的监督微调过程中，主要的目标也是最小化经验损失，由下式进行定义：<script type="math/tex">\min_{\sigma,M^*}\frac{1}{|T|}\sum_{i=1}^{|T|}\mathcal{L}(M^*(\sigma_s(q_i,T_i,C_i,D_i))</script><ul>
<li>其中，$\mathcal{L}$表示模型下一个token预测的损失，将预测的列表和表名词与实际的基础真值名称进行比较</li>
</ul>
</li>
</ul>
<h3 id="2-2-2-SQL生成微调（SQL-Generation-Fine-tuning）"><a href="#2-2-2-SQL生成微调（SQL-Generation-Fine-tuning）" class="headerlink" title="2.2.2 SQL生成微调（SQL Generation Fine-tuning）"></a>2.2.2 SQL生成微调（SQL Generation Fine-tuning）</h3><ul>
<li>当确定了用于生成SQL的适当表之后，下一步就是利用一个基于问题和正确表的模式构造SQL查询模型。由于我们已经使用模式链接模块确定了可能正确的表，因此不需要在SQL生成模型的输入中包含所有表。</li>
<li>与之前微调llm的方法相反，我们从训练数据集$T=\{(q_i.s_i,D_i)\}$中提取与基本真实SQL查询对应的相关表。然后对LLM进行微调，同时最小化下面的损失函数：<script type="math/tex">\min_{\sigma,M^*}\frac{1}{|T|}\sum_{i=1}^{|T|}\mathcal{L}(M^*(\sigma_g(q_i,T_i,s_i))</script><ul>
<li>其中损失函数和2.1定义的是一样的，文本到sql训练过程的这种分解允许llm以单一目标进行训练。通过分离模式链接和SQL查询生成任务，我们改进了训练过程，实现了更集中、更有效的微调。</li>
</ul>
</li>
</ul>
<h1 id="3-Limitations"><a href="#3-Limitations" class="headerlink" title="3. Limitations"></a>3. Limitations</h1><ul>
<li>本文主要侧重于增强文本到sql任务的小型和大型语言模型的性能。然而，各种模式链接技术仍有进一步研究和比较的余地。将检索方法或上下文学习等方法与更大的模型(如GPT-4)一起应用于模式链接任务时，可以对确定最有效的模式链接方法产生有价值的见解。</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/LLMs-sql/" style="color: #00a596">LLMs sql</a>
        </span>
        
    </div>
    <a href="/2024/07/12/DTS-SQL-Decomposed-Text-to-SQL-with-Small-Large-Language-Models/" class="go-post">阅读全文</a>
</div>


        <div class="page-current">
    
    <span class="current">1</span>
    
    
    
    
</div>

    </div>
    
    <div id="home-card">
        <div id="card-style">
    <div id="card-div">
        <div class="avatar">
            <img src="/images/avatar.jpg" alt="avatar" />
        </div>
        <div class="name">Rick</div>
        <div class="description">
            <p>Description<br>…</p>

        </div>
        
        
        <div class="friend-links">
            
            <div class="friend-link">
                <a target="_blank" rel="noopener" href="https://argvchs.github.io">Argvchs</a>
            </div>
            
        </div>
        
    </div>
</div>

    </div>
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2024 XuSibo&#39;s Blog
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Rick
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
</body>
</html>
